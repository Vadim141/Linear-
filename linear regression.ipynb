{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1     0\n",
      "0  23.98  6.459  11.8\n",
      "1  21.52  6.193  11.0\n",
      "2   7.74  6.750  23.7\n",
      "3   4.81  7.249  35.4\n",
      "4  18.06  5.454  15.2\n",
      "5   5.90  6.487  24.4\n",
      "6   2.94  6.998  33.4\n",
      "7   6.36  7.163  31.6\n",
      "8  17.44  6.749  13.4\n",
      "9   4.56  6.975  34.9\n",
      "[23.98   6.459]\n",
      "Shape of train features: (354, 2)\n",
      "Shape of train targets: (354, 1)\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Parse and visualize data\n",
    "# parse train data: read CSV files with train features (train_x) and train targets (train_y)\n",
    "x_train = pd.read_csv(\"D:\\\\Dataset\\\\train\\\\train_x.csv\", header=None)\n",
    "y_train = pd.read_csv(\"D:\\\\Dataset\\\\train\\\\train_y.csv\", header=None)\n",
    "\n",
    "# show first 10 samples\n",
    "print(pd.concat([x_train, y_train], axis=1).head(10))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "print(\"Shape of train features:\", x_train.shape)\n",
    "print(\"Shape of train targets:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Prototypes.\n",
    "\n",
    "# In this demo we will use linear regression to predict targets from features.\n",
    "# In linear regression model with parameters thetas \n",
    "# the prediction y is calculated from features x using linear combination of x and thetas.\n",
    "# For example, for the case of 2 features: \n",
    "# y = theta_0 * x_o + theta_1 * x_1\n",
    "\n",
    "# Let's define some helper functions\n",
    "\n",
    "def predict_fn(x, thetas):\n",
    "    '''\n",
    "    Predict target from features x using parameters thetas and linear regression\n",
    "    \n",
    "    param x: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return y_hat: predicted scalar value for each input samples, shape Nx1\n",
    "    '''    \n",
    "    # TODO: calculate y_hat using linear regression\n",
    "    y_hat = np.zeros((x.shape[0], 1))\n",
    "    for i in range(len(x)):\n",
    "        y_hat[i] = thetas[0] * x[i][0] + thetas[1] * x[i][1]\n",
    "    #print(y_hat)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def loss_fn(x_train, y_train, thetas):\n",
    "    '''\n",
    "    Calculate average loss value for train dataset (x_train, y_train).\n",
    "    \n",
    "    param x_train: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param y_train: input tagrets, shape Nx1\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return loss: predicted scalar value for each input samples, shape Mx1\n",
    "    '''\n",
    "    y_predicted = predict_fn(x_train, thetas)    \n",
    "    loss = np.mean(np.power(y_train - y_predicted, 2))   \n",
    "    return loss\n",
    "\n",
    "\n",
    "def gradient_fn(x_train, y_train, thetas):\n",
    "    '''\n",
    "    Calculate gradient value for linear regression.\n",
    "    \n",
    "    param x_train: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param y_train: input tagrets, shape Nx1\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return g: predicted scalar value for each input samples, shape Mx1\n",
    "    '''  \n",
    "    # TODO: calculate vector gradient\n",
    "    g = np.zeros_like(thetas)\n",
    "    for i in range(len(x_train)):\n",
    "        g[0] += (-2 * y_train[i] * x_train[i][0] + 2 * np.power(x_train[i][0], 2) * thetas[0]\n",
    "        - 2 * x_train[i][0] * x_train[i][1] * thetas[1])\n",
    "        g[1] +=  (2 * y_train[i] * x_train[i][1] + 2 * np.power(x_train[i][1], 2) * thetas[1] \n",
    "        - 2 * x_train[i][0] * x_train[i][1] * thetas[0])\n",
    "    g[0] /= len(x_train)\n",
    "    g[1] /= len(x_train)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training:   0%|                                                                              | 0/10000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Training:   0%|                                                            | 0/10000 [00:00<?, ?it/s, loss_val=1.07e+3]\u001b[A\u001b[A\u001b[A[[-899.87179718]\n",
      " [ 457.67458945]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   0%|                                   | 0/10000 [00:00<?, ?it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-895.44731916]\n",
      " [ 455.94146647]]\n",
      "[[-891.04381042]\n",
      " [ 454.2164498 ]]\n",
      "[[-886.66117176]\n",
      " [ 452.4995011 ]]\n",
      "[[-882.29930441]\n",
      " [ 450.79058222]]\n",
      "[[-877.95811011]\n",
      " [ 449.08965519]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   0%|                           | 6/10000 [00:00<03:06, 53.60it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-873.63749103]\n",
      " [ 447.39668223]]\n",
      "[[-869.33734982]\n",
      " [ 445.71162571]]\n",
      "[[-865.05758958]\n",
      " [ 444.0344482 ]]\n",
      "[[-860.79811389]\n",
      " [ 442.36511245]]\n",
      "[[-856.55882677]\n",
      " [ 440.70358137]]\n",
      "[[-852.33963268]\n",
      " [ 439.04981806]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   0%|                          | 12/10000 [00:00<03:01, 55.08it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-848.14043656]\n",
      " [ 437.4037858 ]]\n",
      "[[-843.96114378]\n",
      " [ 435.76544801]]\n",
      "[[-839.80166018]\n",
      " [ 434.13476832]]\n",
      "[[-835.66189201]\n",
      " [ 432.51171052]]\n",
      "[[-831.541746  ]\n",
      " [ 430.89623856]]\n",
      "[[-827.4411293 ]\n",
      " [ 429.28831658]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   0%|                          | 18/10000 [00:00<02:57, 56.16it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-823.3599495 ]\n",
      " [ 427.68790888]]\n",
      "[[-819.29811464]\n",
      " [ 426.09497991]]\n",
      "[[-815.25553318]\n",
      " [ 424.50949433]]\n",
      "[[-811.23211403]\n",
      " [ 422.93141693]]\n",
      "[[-807.22776652]\n",
      " [ 421.36071268]]\n",
      "[[-803.2424004 ]\n",
      " [ 419.79734672]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   0%|                          | 24/10000 [00:00<02:55, 56.94it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-799.27592586]\n",
      " [ 418.24128436]]\n",
      "[[-795.32825351]\n",
      " [ 416.69249105]]\n",
      "[[-791.3992944 ]\n",
      " [ 415.15093242]]\n",
      "[[-787.48895997]\n",
      " [ 413.61657427]]\n",
      "[[-783.5971621 ]\n",
      " [ 412.08938255]]\n",
      "[[-779.72381308]\n",
      " [ 410.56932338]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   0%|                          | 30/10000 [00:00<02:53, 57.60it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-775.86882562]\n",
      " [ 409.05636302]]\n",
      "[[-772.03211284]\n",
      " [ 407.55046791]]\n",
      "[[-768.21358827]\n",
      " [ 406.05160465]]\n",
      "[[-764.41316584]\n",
      " [ 404.55973998]]\n",
      "[[-760.63075991]\n",
      " [ 403.07484082]]\n",
      "[[-756.86628523]\n",
      " [ 401.59687423]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   0%|                          | 36/10000 [00:00<02:53, 57.47it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-753.11965696]\n",
      " [ 400.12580742]]\n",
      "[[-749.39079064]\n",
      " [ 398.66160778]]\n",
      "[[-745.67960224]\n",
      " [ 397.20424284]]\n",
      "[[-741.98600811]\n",
      " [ 395.75368027]]\n",
      "[[-738.309925  ]\n",
      " [ 394.30988792]]\n",
      "[[-734.65127006]\n",
      " [ 392.87283377]]\n",
      "[[-731.00996082]\n",
      " [ 391.44248597]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   0%|                          | 43/10000 [00:00<02:50, 58.34it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-727.38591521]\n",
      " [ 390.0188128 ]]\n",
      "[[-723.77905153]\n",
      " [ 388.60178271]]\n",
      "[[-720.1892885 ]\n",
      " [ 387.19136428]]\n",
      "[[-716.61654519]\n",
      " [ 385.78752626]]\n",
      "[[-713.06074108]\n",
      " [ 384.39023752]]\n",
      "[[-709.52179602]\n",
      " [ 382.99946712]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   0%|▏                         | 49/10000 [00:00<02:49, 58.66it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-705.99963022]\n",
      " [ 381.61518421]]\n",
      "[[-702.49416431]\n",
      " [ 380.23735814]]\n",
      "[[-699.00531926]\n",
      " [ 378.86595838]]\n",
      "[[-695.53301642]\n",
      " [ 377.50095453]]\n",
      "[[-692.07717752]\n",
      " [ 376.14231636]]\n",
      "[[-688.63772467]\n",
      " [ 374.79001377]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▏                         | 55/10000 [00:00<02:50, 58.38it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-685.21458032]\n",
      " [ 373.4440168 ]]\n",
      "[[-681.80766731]\n",
      " [ 372.10429565]]\n",
      "[[-678.41690884]\n",
      " [ 370.77082064]]\n",
      "[[-675.04222847]\n",
      " [ 369.44356224]]\n",
      "[[-671.68355013]\n",
      " [ 368.12249105]]\n",
      "[[-668.34079809]\n",
      " [ 366.80757783]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▏                         | 61/10000 [00:01<02:51, 58.01it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-665.013897  ]\n",
      " [ 365.49879345]]\n",
      "[[-661.70277185]\n",
      " [ 364.19610895]]\n",
      "[[-658.40734801]\n",
      " [ 362.89949548]]\n",
      "[[-655.12755116]\n",
      " [ 361.60892433]]\n",
      "[[-651.86330739]\n",
      " [ 360.32436695]]\n",
      "[[-648.61454308]\n",
      " [ 359.04579489]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▏                         | 67/10000 [00:01<02:51, 57.81it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-645.38118499]\n",
      " [ 357.77317987]]\n",
      "[[-642.16316024]\n",
      " [ 356.50649371]]\n",
      "[[-638.96039625]\n",
      " [ 355.24570839]]\n",
      "[[-635.77282084]\n",
      " [ 353.990796  ]]\n",
      "[[-632.60036211]\n",
      " [ 352.74172878]]\n",
      "[[-629.44294856]\n",
      " [ 351.4984791 ]]\n",
      "[[-626.30050899]\n",
      " [ 350.26101945]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▏                         | 74/10000 [00:01<02:49, 58.56it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-623.17297254]\n",
      " [ 349.02932246]]\n",
      "[[-620.0602687 ]\n",
      " [ 347.80336088]]\n",
      "[[-616.96232728]\n",
      " [ 346.5831076 ]]\n",
      "[[-613.87907844]\n",
      " [ 345.36853563]]\n",
      "[[-610.81045264]\n",
      " [ 344.15961811]]\n",
      "[[-607.75638071]\n",
      " [ 342.95632831]]\n",
      "[[-604.71679377]\n",
      " [ 341.75863962]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▏                         | 81/10000 [00:01<02:48, 58.94it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-601.69162328]\n",
      " [ 340.56652556]]\n",
      "[[-598.68080103]\n",
      " [ 339.37995978]]\n",
      "[[-595.68425913]\n",
      " [ 338.19891605]]\n",
      "[[-592.70193   ]\n",
      " [ 337.02336825]]\n",
      "[[-589.73374639]\n",
      " [ 335.85329042]]\n",
      "[[-586.77964138]\n",
      " [ 334.68865668]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▏                         | 87/10000 [00:01<02:49, 58.40it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-583.83954834]\n",
      " [ 333.5294413 ]]\n",
      "[[-580.91340097]\n",
      " [ 332.37561867]]\n",
      "[[-578.00113329]\n",
      " [ 331.22716329]]\n",
      "[[-575.10267962]\n",
      " [ 330.08404979]]\n",
      "[[-572.21797459]\n",
      " [ 328.94625291]]\n",
      "[[-569.34695315]\n",
      " [ 327.81374752]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▏                         | 93/10000 [00:01<02:50, 57.97it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-566.48955055]\n",
      " [ 326.6865086 ]]\n",
      "[[-563.64570235]\n",
      " [ 325.56451126]]\n",
      "[[-560.81534441]\n",
      " [ 324.44773071]]\n",
      "[[-557.99841291]\n",
      " [ 323.3361423 ]]\n",
      "[[-555.1948443 ]\n",
      " [ 322.22972148]]\n",
      "[[-552.40457537]\n",
      " [ 321.12844383]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▎                         | 99/10000 [00:01<02:50, 58.07it/s, loss_val=1066.8534, thetas=-0.9169 0.3017]\u001b[A\u001b[A\u001b[A[[-549.62754317]\n",
      " [ 320.03228502]]\n",
      "[[-546.86368508]\n",
      " [ 318.94122085]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▎                         | 99/10000 [00:01<02:50, 58.07it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-544.11293875]\n",
      " [ 317.85522726]]\n",
      "[[-541.37524215]\n",
      " [ 316.77428025]]\n",
      "[[-538.65053352]\n",
      " [ 315.69835599]]\n",
      "[[-535.93875141]\n",
      " [ 314.62743072]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▎                        | 105/10000 [00:01<02:53, 57.14it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-533.23983466]\n",
      " [ 313.56148081]]\n",
      "[[-530.55372238]\n",
      " [ 312.50048274]]\n",
      "[[-527.88035399]\n",
      " [ 311.4444131 ]]\n",
      "[[-525.21966919]\n",
      " [ 310.39324861]]\n",
      "[[-522.57160796]\n",
      " [ 309.34696606]]\n",
      "[[-519.93611056]\n",
      " [ 308.30554238]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▎                        | 111/10000 [00:01<02:51, 57.81it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-517.31311756]\n",
      " [ 307.26895461]]\n",
      "[[-514.70256977]\n",
      " [ 306.23717987]]\n",
      "[[-512.10440831]\n",
      " [ 305.21019543]]\n",
      "[[-509.51857458]\n",
      " [ 304.18797863]]\n",
      "[[-506.94501023]\n",
      " [ 303.17050694]]\n",
      "[[-504.38365722]\n",
      " [ 302.15775794]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▎                        | 117/10000 [00:02<02:50, 58.12it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-501.83445775]\n",
      " [ 301.14970928]]\n",
      "[[-499.29735432]\n",
      " [ 300.14633876]]\n",
      "[[-496.7722897 ]\n",
      " [ 299.14762427]]\n",
      "[[-494.25920691]\n",
      " [ 298.15354378]]\n",
      "[[-491.75804927]\n",
      " [ 297.16407541]]\n",
      "[[-489.26876033]\n",
      " [ 296.17919734]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▎                        | 123/10000 [00:02<02:49, 58.27it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-486.79128395]\n",
      " [ 295.19888788]]\n",
      "[[-484.32556423]\n",
      " [ 294.22312544]]\n",
      "[[-481.87154554]\n",
      " [ 293.25188852]]\n",
      "[[-479.4291725 ]\n",
      " [ 292.28515572]]\n",
      "[[-476.99839001]\n",
      " [ 291.32290577]]\n",
      "[[-474.57914323]\n",
      " [ 290.36511746]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▎                        | 129/10000 [00:02<02:49, 58.28it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-472.17137758]\n",
      " [ 289.41176971]]\n",
      "[[-469.77503872]\n",
      " [ 288.46284154]]\n",
      "[[-467.39007258]\n",
      " [ 287.51831204]]\n",
      "[[-465.01642535]\n",
      " [ 286.57816043]]\n",
      "[[-462.65404347]\n",
      " [ 285.64236602]]\n",
      "[[-460.30287364]\n",
      " [ 284.7109082 ]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▎                        | 135/10000 [00:02<02:49, 58.28it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-457.9628628 ]\n",
      " [ 283.78376649]]\n",
      "[[-455.63395814]\n",
      " [ 282.86092048]]\n",
      "[[-453.31610711]\n",
      " [ 281.94234987]]\n",
      "[[-451.00925742]\n",
      " [ 281.02803445]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-448.71335699]\n",
      " [ 280.1179541 ]]\n",
      "[[-446.42835402]\n",
      " [ 279.21208881]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▎                        | 141/10000 [00:02<02:49, 58.28it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-444.15419695]\n",
      " [ 278.31041866]]\n",
      "[[-441.89083445]\n",
      " [ 277.41292382]]\n",
      "[[-439.63821544]\n",
      " [ 276.51958455]]\n",
      "[[-437.39628909]\n",
      " [ 275.63038121]]\n",
      "[[-435.16500479]\n",
      " [ 274.74529426]]\n",
      "[[-432.9443122 ]\n",
      " [ 273.86430423]]\n",
      "[[-430.73416119]\n",
      " [ 272.98739177]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   1%|▎                        | 148/10000 [00:02<02:47, 58.75it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-428.53450188]\n",
      " [ 272.11453759]]\n",
      "[[-426.34528463]\n",
      " [ 271.24572253]]\n",
      "[[-424.16646001]\n",
      " [ 270.38092749]]\n",
      "[[-421.99797886]\n",
      " [ 269.52013347]]\n",
      "[[-419.83979223]\n",
      " [ 268.66332156]]\n",
      "[[-417.69185141]\n",
      " [ 267.81047293]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▍                        | 154/10000 [00:02<02:48, 58.44it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-415.55410792]\n",
      " [ 266.96156887]]\n",
      "[[-413.42651349]\n",
      " [ 266.11659072]]\n",
      "[[-411.30902011]\n",
      " [ 265.27551992]]\n",
      "[[-409.20157998]\n",
      " [ 264.43833803]]\n",
      "[[-407.10414553]\n",
      " [ 263.60502664]]\n",
      "[[-405.0166694 ]\n",
      " [ 262.77556748]]\n",
      "[[-402.93910449]\n",
      " [ 261.94994233]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▍                        | 161/10000 [00:02<02:46, 59.16it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-400.87140388]\n",
      " [ 261.12813307]]\n",
      "[[-398.8135209 ]\n",
      " [ 260.31012167]]\n",
      "[[-396.76540909]\n",
      " [ 259.49589019]]\n",
      "[[-394.72702222]\n",
      " [ 258.68542075]]\n",
      "[[-392.69831427]\n",
      " [ 257.87869558]]\n",
      "[[-390.67923944]\n",
      " [ 257.07569698]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▍                        | 167/10000 [00:02<02:46, 59.07it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-388.66975214]\n",
      " [ 256.27640734]]\n",
      "[[-386.669807  ]\n",
      " [ 255.48080913]]\n",
      "[[-384.67935888]\n",
      " [ 254.68888491]]\n",
      "[[-382.69836282]\n",
      " [ 253.90061731]]\n",
      "[[-380.72677411]\n",
      " [ 253.11598905]]\n",
      "[[-378.76454823]\n",
      " [ 252.33498294]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▍                        | 173/10000 [00:02<02:46, 59.18it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-376.81164087]\n",
      " [ 251.55758186]]\n",
      "[[-374.86800794]\n",
      " [ 250.78376876]]\n",
      "[[-372.93360555]\n",
      " [ 250.0135267 ]]\n",
      "[[-371.00839002]\n",
      " [ 249.24683879]]\n",
      "[[-369.09231789]\n",
      " [ 248.48368824]]\n",
      "[[-367.18534587]\n",
      " [ 247.72405834]]\n",
      "[[-365.28743091]\n",
      " [ 246.96793245]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▍                        | 180/10000 [00:03<02:44, 59.63it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-363.39853016]\n",
      " [ 246.21529401]]\n",
      "[[-361.51860095]\n",
      " [ 245.46612653]]\n",
      "[[-359.64760083]\n",
      " [ 244.72041362]]\n",
      "[[-357.78548755]\n",
      " [ 243.97813895]]\n",
      "[[-355.93221906]\n",
      " [ 243.23928627]]\n",
      "[[-354.08775349]\n",
      " [ 242.50383941]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▍                        | 186/10000 [00:03<02:48, 58.19it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-352.25204921]\n",
      " [ 241.77178229]]\n",
      "[[-350.42506474]\n",
      " [ 241.04309887]]\n",
      "[[-348.60675882]\n",
      " [ 240.31777322]]\n",
      "[[-346.79709039]\n",
      " [ 239.59578947]]\n",
      "[[-344.99601858]\n",
      " [ 238.87713183]]\n",
      "[[-343.20350269]\n",
      " [ 238.16178458]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▍                        | 192/10000 [00:03<02:49, 57.87it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-341.41950225]\n",
      " [ 237.44973208]]\n",
      "[[-339.64397696]\n",
      " [ 236.74095876]]\n",
      "[[-337.87688672]\n",
      " [ 236.03544912]]\n",
      "[[-336.1181916 ]\n",
      " [ 235.33318775]]\n",
      "[[-334.36785189]\n",
      " [ 234.63415929]]\n",
      "[[-332.62582803]\n",
      " [ 233.93834848]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▍                        | 198/10000 [00:03<02:50, 57.33it/s, loss_val=722.3359, thetas=-0.2092 -0.0805]\u001b[A\u001b[A\u001b[A[[-330.89208069]\n",
      " [ 233.24574009]]\n",
      "[[-329.1665707 ]\n",
      " [ 232.55631901]]\n",
      "[[-327.44925907]\n",
      " [ 231.87007017]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▌                         | 198/10000 [00:03<02:50, 57.33it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-325.74010701]\n",
      " [ 231.18697858]]\n",
      "[[-324.03907591]\n",
      " [ 230.50702932]]\n",
      "[[-322.34612734]\n",
      " [ 229.83020755]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▌                         | 204/10000 [00:03<02:52, 56.79it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-320.66122305]\n",
      " [ 229.15649849]]\n",
      "[[-318.98432497]\n",
      " [ 228.48588743]]\n",
      "[[-317.31539522]\n",
      " [ 227.81835974]]\n",
      "[[-315.65439609]\n",
      " [ 227.15390084]]\n",
      "[[-314.00129005]\n",
      " [ 226.49249623]]\n",
      "[[-312.35603975]\n",
      " [ 225.83413149]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▌                         | 210/10000 [00:03<02:50, 57.40it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-310.71860802]\n",
      " [ 225.17879225]]\n",
      "[[-309.08895785]\n",
      " [ 224.52646422]]\n",
      "[[-307.46705242]\n",
      " [ 223.87713316]]\n",
      "[[-305.85285509]\n",
      " [ 223.23078492]]\n",
      "[[-304.24632937]\n",
      " [ 222.58740541]]\n",
      "[[-302.64743897]\n",
      " [ 221.94698059]]\n",
      "[[-301.05614774]\n",
      " [ 221.30949652]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▌                         | 217/10000 [00:03<02:47, 58.26it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-299.47241974]\n",
      " [ 220.67493929]]\n",
      "[[-297.89621917]\n",
      " [ 220.04329508]]\n",
      "[[-296.3275104 ]\n",
      " [ 219.41455012]]\n",
      "[[-294.766258  ]\n",
      " [ 218.78869071]]\n",
      "[[-293.21242666]\n",
      " [ 218.16570323]]\n",
      "[[-291.66598127]\n",
      " [ 217.54557411]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▌                         | 223/10000 [00:03<02:48, 57.93it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-290.12688689]\n",
      " [ 216.92828983]]\n",
      "[[-288.59510871]\n",
      " [ 216.31383697]]\n",
      "[[-287.07061213]\n",
      " [ 215.70220214]]\n",
      "[[-285.55336269]\n",
      " [ 215.09337203]]\n",
      "[[-284.04332608]\n",
      " [ 214.48733339]]\n",
      "[[-282.54046817]\n",
      " [ 213.88407303]]\n",
      "[[-281.044755  ]\n",
      " [ 213.28357783]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▌                         | 230/10000 [00:03<02:45, 58.88it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-279.55615276]\n",
      " [ 212.68583472]]\n",
      "[[-278.07462778]\n",
      " [ 212.0908307 ]]\n",
      "[[-276.60014659]\n",
      " [ 211.49855283]]\n",
      "[[-275.13267584]\n",
      " [ 210.90898824]]\n",
      "[[-273.67218237]\n",
      " [ 210.3221241 ]]\n",
      "[[-272.21863314]\n",
      " [ 209.73794766]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▌                         | 236/10000 [00:04<02:47, 58.36it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-270.77199531]\n",
      " [ 209.15644622]]\n",
      "[[-269.33223617]\n",
      " [ 208.57760714]]\n",
      "[[-267.89932316]\n",
      " [ 208.00141785]]\n",
      "[[-266.47322388]\n",
      " [ 207.42786583]]\n",
      "[[-265.05390609]\n",
      " [ 206.85693862]]\n",
      "[[-263.64133771]\n",
      " [ 206.28862383]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▋                         | 242/10000 [00:04<02:49, 57.54it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-262.23548677]\n",
      " [ 205.7229091 ]]\n",
      "[[-260.83632151]\n",
      " [ 205.15978216]]\n",
      "[[-259.44381028]\n",
      " [ 204.59923079]]\n",
      "[[-258.05792159]\n",
      " [ 204.04124282]]\n",
      "[[-256.67862409]\n",
      " [ 203.48580613]]\n",
      "[[-255.30588661]\n",
      " [ 202.93290869]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   2%|▋                         | 248/10000 [00:04<02:50, 57.26it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-253.93967809]\n",
      " [ 202.38253848]]\n",
      "[[-252.57996764]\n",
      " [ 201.83468359]]\n",
      "[[-251.2267245 ]\n",
      " [ 201.28933211]]\n",
      "[[-249.87991808]\n",
      " [ 200.74647224]]\n",
      "[[-248.5395179]\n",
      " [ 200.2060922]]\n",
      "[[-247.20549366]\n",
      " [ 199.66818027]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▋                         | 254/10000 [00:04<02:49, 57.40it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-245.87781518]\n",
      " [ 199.1327248 ]]\n",
      "[[-244.55645242]\n",
      " [ 198.59971419]]\n",
      "[[-243.24137551]\n",
      " [ 198.06913688]]\n",
      "[[-241.93255468]\n",
      " [ 197.54098139]]\n",
      "[[-240.62996035]\n",
      " [ 197.01523628]]\n",
      "[[-239.33356303]\n",
      " [ 196.49189015]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▋                         | 260/10000 [00:04<02:49, 57.34it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-238.04333341]\n",
      " [ 195.97093169]]\n",
      "[[-236.7592423]\n",
      " [ 195.4523496]]\n",
      "[[-235.48126064]\n",
      " [ 194.93613268]]\n",
      "[[-234.20935952]\n",
      " [ 194.42226974]]\n",
      "[[-232.94351017]\n",
      " [ 193.91074967]]\n",
      "[[-231.68368395]\n",
      " [ 193.4015614 ]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▋                         | 266/10000 [00:04<02:49, 57.29it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-230.42985235]\n",
      " [ 192.89469393]]\n",
      "[[-229.181987  ]\n",
      " [ 192.39013628]]\n",
      "[[-227.94005968]\n",
      " [ 191.88787755]]\n",
      "[[-226.70404227]\n",
      " [ 191.38790689]]\n",
      "[[-225.47390681]\n",
      " [ 190.89021348]]\n",
      "[[-224.24962547]\n",
      " [ 190.39478657]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▋                         | 272/10000 [00:04<02:49, 57.42it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-223.03117053]\n",
      " [ 189.90161546]]\n",
      "[[-221.81851443]\n",
      " [ 189.41068949]]\n",
      "[[-220.61162972]\n",
      " [ 188.92199806]]\n",
      "[[-219.4104891 ]\n",
      " [ 188.43553062]]\n",
      "[[-218.21506536]\n",
      " [ 187.95127666]]\n",
      "[[-217.02533147]\n",
      " [ 187.46922573]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▋                         | 278/10000 [00:04<02:47, 58.01it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-215.84126049]\n",
      " [ 186.98936743]]\n",
      "[[-214.66282562]\n",
      " [ 186.5116914 ]]\n",
      "[[-213.49000019]\n",
      " [ 186.03618733]]\n",
      "[[-212.32275766]\n",
      " [ 185.56284498]]\n",
      "[[-211.1610716 ]\n",
      " [ 185.09165412]]\n",
      "[[-210.00491571]\n",
      " [ 184.62260461]]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   3%|▋                         | 284/10000 [00:04<02:46, 58.26it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-208.85426383]\n",
      " [ 184.15568632]]\n",
      "[[-207.7090899 ]\n",
      " [ 183.69088919]]\n",
      "[[-206.56936801]\n",
      " [ 183.22820322]]\n",
      "[[-205.43507234]\n",
      " [ 182.76761842]]\n",
      "[[-204.30617723]\n",
      " [ 182.30912487]]\n",
      "[[-203.1826571 ]\n",
      " [ 181.85271271]]\n",
      "[[-202.06448653]\n",
      " [ 181.3983721 ]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▊                         | 291/10000 [00:04<02:44, 59.03it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-200.95164019]\n",
      " [ 180.94609326]]\n",
      "[[-199.84409289]\n",
      " [ 180.49586645]]\n",
      "[[-198.74181955]\n",
      " [ 180.04768199]]\n",
      "[[-197.64479521]\n",
      " [ 179.60153023]]\n",
      "[[-196.55299503]\n",
      " [ 179.15740157]]\n",
      "[[-195.46639428]\n",
      " [ 178.71528646]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▊                         | 297/10000 [00:05<02:45, 58.63it/s, loss_val=592.3821, thetas=0.2181 -0.3522]\u001b[A\u001b[A\u001b[A[[-194.38496837]\n",
      " [ 178.2751754 ]]\n",
      "[[-193.30869279]\n",
      " [ 177.83705891]]\n",
      "[[-192.23754317]\n",
      " [ 177.40092758]]\n",
      "[[-191.17149525]\n",
      " [ 176.96677205]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▊                         | 297/10000 [00:05<02:45, 58.63it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-190.1105249 ]\n",
      " [ 176.53458297]]\n",
      "[[-189.05460807]\n",
      " [ 176.10435107]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▊                         | 303/10000 [00:05<02:48, 57.42it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-188.00372086]\n",
      " [ 175.6760671 ]]\n",
      "[[-186.95783945]\n",
      " [ 175.24972187]]\n",
      "[[-185.91694016]\n",
      " [ 174.82530623]]\n",
      "[[-184.88099941]\n",
      " [ 174.40281105]]\n",
      "[[-183.84999373]\n",
      " [ 173.98222728]]\n",
      "[[-182.82389976]\n",
      " [ 173.5635459 ]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▊                         | 309/10000 [00:05<02:48, 57.68it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-181.80269427]\n",
      " [ 173.14675791]]\n",
      "[[-180.7863541 ]\n",
      " [ 172.73185438]]\n",
      "[[-179.77485625]\n",
      " [ 172.31882642]]\n",
      "[[-178.76817778]\n",
      " [ 171.90766516]]\n",
      "[[-177.7662959]\n",
      " [ 171.4983618]]\n",
      "[[-176.76918789]\n",
      " [ 171.09090757]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▊                         | 315/10000 [00:05<02:46, 58.20it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-175.77683118]\n",
      " [ 170.68529373]]\n",
      "[[-174.78920326]\n",
      " [ 170.2815116 ]]\n",
      "[[-173.80628176]\n",
      " [ 169.87955252]]\n",
      "[[-172.82804441]\n",
      " [ 169.4794079 ]]\n",
      "[[-171.85446903]\n",
      " [ 169.08106917]]\n",
      "[[-170.88553357]\n",
      " [ 168.6845278 ]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▊                         | 321/10000 [00:05<02:47, 57.72it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-169.92121605]\n",
      " [ 168.2897753 ]]\n",
      "[[-168.96149464]\n",
      " [ 167.89680324]]\n",
      "[[-168.00634757]\n",
      " [ 167.5056032 ]]\n",
      "[[-167.05575319]\n",
      " [ 167.11616683]]\n",
      "[[-166.10968996]\n",
      " [ 166.7284858 ]]\n",
      "[[-165.16813643]\n",
      " [ 166.34255181]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▊                         | 327/10000 [00:05<02:47, 57.72it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-164.23107126]\n",
      " [ 165.95835663]]\n",
      "[[-163.29847321]\n",
      " [ 165.57589205]]\n",
      "[[-162.37032113]\n",
      " [ 165.1951499 ]]\n",
      "[[-161.44659398]\n",
      " [ 164.81612204]]\n",
      "[[-160.52727082]\n",
      " [ 164.43880039]]\n",
      "[[-159.61233081]\n",
      " [ 164.06317688]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▊                         | 333/10000 [00:05<02:46, 57.89it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-158.7017532 ]\n",
      " [ 163.68924352]]\n",
      "[[-157.79551734]\n",
      " [ 163.31699231]]\n",
      "[[-156.89360269]\n",
      " [ 162.94641533]]\n",
      "[[-155.99598879]\n",
      " [ 162.57750466]]\n",
      "[[-155.10265528]\n",
      " [ 162.21025244]]\n",
      "[[-154.21358192]\n",
      " [ 161.84465084]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▉                         | 339/10000 [00:05<02:46, 58.01it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-153.32874853]\n",
      " [ 161.48069208]]\n",
      "[[-152.44813505]\n",
      " [ 161.1183684 ]]\n",
      "[[-151.57172151]\n",
      " [ 160.75767208]]\n",
      "[[-150.69948803]\n",
      " [ 160.39859544]]\n",
      "[[-149.83141482]\n",
      " [ 160.04113084]]\n",
      "[[-148.96748219]\n",
      " [ 159.68527067]]\n",
      "[[-148.10767055]\n",
      " [ 159.33100735]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   3%|▉                         | 346/10000 [00:05<02:43, 58.99it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-147.25196039]\n",
      " [ 158.97833336]]\n",
      "[[-146.4003323 ]\n",
      " [ 158.62724118]]\n",
      "[[-145.55276696]\n",
      " [ 158.27772337]]\n",
      "[[-144.70924514]\n",
      " [ 157.92977247]]\n",
      "[[-143.86974771]\n",
      " [ 157.58338111]]\n",
      "[[-143.0342556 ]\n",
      " [ 157.23854192]]\n",
      "[[-142.20274988]\n",
      " [ 156.89524757]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|▉                         | 353/10000 [00:06<02:42, 59.40it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-141.37521166]\n",
      " [ 156.55349078]]\n",
      "[[-140.55162218]\n",
      " [ 156.21326429]]\n",
      "[[-139.73196273]\n",
      " [ 155.87456088]]\n",
      "[[-138.91621473]\n",
      " [ 155.53737336]]\n",
      "[[-138.10435966]\n",
      " [ 155.20169457]]\n",
      "[[-137.29637908]\n",
      " [ 154.86751741]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|▉                         | 359/10000 [00:06<02:43, 58.89it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-136.49225467]\n",
      " [ 154.53483477]]\n",
      "[[-135.69196818]\n",
      " [ 154.20363962]]\n",
      "[[-134.89550143]\n",
      " [ 153.87392493]]\n",
      "[[-134.10283634]\n",
      " [ 153.54568371]]\n",
      "[[-133.31395493]\n",
      " [ 153.21890902]]\n",
      "[[-132.52883929]\n",
      " [ 152.89359393]]\n",
      "[[-131.74747159]\n",
      " [ 152.56973156]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|▉                         | 366/10000 [00:06<02:42, 59.33it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-130.96983408]\n",
      " [ 152.24731505]]\n",
      "[[-130.19590913]\n",
      " [ 151.92633758]]\n",
      "[[-129.42567915]\n",
      " [ 151.60679236]]\n",
      "[[-128.65912665]\n",
      " [ 151.28867263]]\n",
      "[[-127.89623423]\n",
      " [ 150.97197167]]\n",
      "[[-127.13698457]\n",
      " [ 150.65668278]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|▉                         | 372/10000 [00:06<02:43, 58.84it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-126.38136042]\n",
      " [ 150.3427993 ]]\n",
      "[[-125.62934463]\n",
      " [ 150.0303146 ]]\n",
      "[[-124.8809201 ]\n",
      " [ 149.71922209]]\n",
      "[[-124.13606985]\n",
      " [ 149.40951519]]\n",
      "[[-123.39477696]\n",
      " [ 149.10118736]]\n",
      "[[-122.65702459]\n",
      " [ 148.7942321 ]]\n",
      "[[-121.92279599]\n",
      " [ 148.48864294]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|▉                         | 379/10000 [00:06<02:42, 59.29it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-121.19207446]\n",
      " [ 148.18441343]]\n",
      "[[-120.46484342]\n",
      " [ 147.88153715]]\n",
      "[[-119.74108633]\n",
      " [ 147.58000773]]\n",
      "[[-119.02078677]\n",
      " [ 147.2798188 ]]\n",
      "[[-118.30392836]\n",
      " [ 146.98096405]]\n",
      "[[-117.59049481]\n",
      " [ 146.68343718]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█                         | 385/10000 [00:06<02:43, 58.64it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-116.88046991]\n",
      " [ 146.38723192]]\n",
      "[[-116.17383754]\n",
      " [ 146.09234205]]\n",
      "[[-115.47058162]\n",
      " [ 145.79876136]]\n",
      "[[-114.77068619]\n",
      " [ 145.50648367]]\n",
      "[[-114.07413533]\n",
      " [ 145.21550284]]\n",
      "[[-113.38091321]\n",
      " [ 144.92581275]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█                         | 391/10000 [00:06<02:43, 58.88it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-112.69100407]\n",
      " [ 144.6374073 ]]\n",
      "[[-112.00439224]\n",
      " [ 144.35028045]]\n",
      "[[-111.32106211]\n",
      " [ 144.06442617]]\n",
      "[[-110.64099814]\n",
      " [ 143.77983844]]\n",
      "[[-109.96418487]\n",
      " [ 143.49651129]]\n",
      "[[-109.29060692]\n",
      " [ 143.21443879]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█                         | 397/10000 [00:06<02:42, 59.05it/s, loss_val=548.1207, thetas=0.4714 -0.5542]\u001b[A\u001b[A\u001b[A[[-108.62024897]\n",
      " [ 142.933615  ]]\n",
      "[[-107.95309578]\n",
      " [ 142.65403405]]\n",
      "[[-107.28913217]\n",
      " [ 142.37569008]]\n",
      "[[-106.62834307]\n",
      " [ 142.09857724]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█                         | 397/10000 [00:06<02:42, 59.05it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-105.97071343]\n",
      " [ 141.82268973]]\n",
      "[[-105.3162283 ]\n",
      " [ 141.54802177]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█                         | 403/10000 [00:06<02:44, 58.47it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-104.66487279]\n",
      " [ 141.27456761]]\n",
      "[[-104.0166321 ]\n",
      " [ 141.00232153]]\n",
      "[[-103.37149148]\n",
      " [ 140.73127784]]\n",
      "[[-102.72943625]\n",
      " [ 140.46143085]]\n",
      "[[-102.09045181]\n",
      " [ 140.19277492]]\n",
      "[[-101.45452362]\n",
      " [ 139.92530445]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█                         | 409/10000 [00:07<02:43, 58.59it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-100.82163722]\n",
      " [ 139.65901383]]\n",
      "[[-100.19177821]\n",
      " [ 139.39389751]]\n",
      "[[-99.56493225]\n",
      " [139.12994995]]\n",
      "[[-98.94108509]\n",
      " [138.86716564]]\n",
      "[[-98.32022253]\n",
      " [138.60553909]]\n",
      "[[-97.70233044]\n",
      " [138.34506484]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█                         | 415/10000 [00:07<02:43, 58.61it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-97.08739477]\n",
      " [138.08573746]]\n",
      "[[-96.4754015 ]\n",
      " [137.82755154]]\n",
      "[[-95.86633673]\n",
      " [137.57050171]]\n",
      "[[-95.26018658]\n",
      " [137.31458259]]\n",
      "[[-94.65693726]\n",
      " [137.05978887]]\n",
      "[[-94.05657504]\n",
      " [136.80611524]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█                         | 421/10000 [00:07<02:42, 58.85it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-93.45908625]\n",
      " [136.55355641]]\n",
      "[[-92.86445729]\n",
      " [136.30210713]]\n",
      "[[-92.27267462]\n",
      " [136.05176217]]\n",
      "[[-91.68372476]\n",
      " [135.80251632]]\n",
      "[[-91.09759431]\n",
      " [135.5543644 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-90.51426993]\n",
      " [135.30730126]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█                         | 427/10000 [00:07<02:43, 58.51it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-89.93373832]\n",
      " [135.06132175]]\n",
      "[[-89.35598627]\n",
      " [134.81642078]]\n",
      "[[-88.78100062]\n",
      " [134.57259325]]\n",
      "[[-88.20876827]\n",
      " [134.32983411]]\n",
      "[[-87.63927619]\n",
      " [134.08813833]]\n",
      "[[-87.07251141]\n",
      " [133.84750088]]\n",
      "[[-86.50846102]\n",
      " [133.60791678]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█▏                        | 434/10000 [00:07<02:41, 59.36it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-85.94711217]\n",
      " [133.36938106]]\n",
      "[[-85.38845207]\n",
      " [133.13188879]]\n",
      "[[-84.83246799]\n",
      " [132.89543504]]\n",
      "[[-84.27914726]\n",
      " [132.66001493]]\n",
      "[[-83.72847728]\n",
      " [132.42562356]]\n",
      "[[-83.1804455 ]\n",
      " [132.19225611]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█▏                        | 440/10000 [00:07<02:41, 59.04it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-82.63503942]\n",
      " [131.95990774]]\n",
      "[[-82.09224663]\n",
      " [131.72857365]]\n",
      "[[-81.55205474]\n",
      " [131.49824906]]\n",
      "[[-81.01445144]\n",
      " [131.26892921]]\n",
      "[[-80.47942449]\n",
      " [131.04060937]]\n",
      "[[-79.94696168]\n",
      " [130.81328482]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   4%|█▏                        | 446/10000 [00:07<02:41, 58.98it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-79.41705088]\n",
      " [130.58695087]]\n",
      "[[-78.88968   ]\n",
      " [130.36160285]]\n",
      "[[-78.36483702]\n",
      " [130.13723613]]\n",
      "[[-77.84250997]\n",
      " [129.91384606]]\n",
      "[[-77.32268695]\n",
      " [129.69142806]]\n",
      "[[-76.80535609]\n",
      " [129.46997754]]\n",
      "[[-76.29050561]\n",
      " [129.24948994]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▏                        | 453/10000 [00:07<02:39, 59.85it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-75.77812375]\n",
      " [129.02996072]]\n",
      "[[-75.26819883]\n",
      " [128.81138537]]\n",
      "[[-74.76071923]\n",
      " [128.5937594 ]]\n",
      "[[-74.25567336]\n",
      " [128.37707832]]\n",
      "[[-73.7530497]\n",
      " [128.1613377]]\n",
      "[[-73.25283678]\n",
      " [127.94653309]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▏                        | 459/10000 [00:07<02:41, 59.02it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-72.7550232 ]\n",
      " [127.73266008]]\n",
      "[[-72.25959759]\n",
      " [127.51971429]]\n",
      "[[-71.76654865]\n",
      " [127.30769135]]\n",
      "[[-71.27586512]\n",
      " [127.09658692]]\n",
      "[[-70.78753582]\n",
      " [126.88639665]]\n",
      "[[-70.30154958]\n",
      " [126.67711625]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▏                        | 465/10000 [00:07<02:41, 58.90it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-69.81789533]\n",
      " [126.46874143]]\n",
      "[[-69.33656202]\n",
      " [126.26126793]]\n",
      "[[-68.85753865]\n",
      " [126.05469149]]\n",
      "[[-68.38081431]\n",
      " [125.8490079 ]]\n",
      "[[-67.90637809]\n",
      " [125.64421294]]\n",
      "[[-67.43421918]\n",
      " [125.44030243]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▏                        | 471/10000 [00:08<02:43, 58.21it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-66.96432678]\n",
      " [125.2372722 ]]\n",
      "[[-66.49669017]\n",
      " [125.03511811]]\n",
      "[[-66.03129867]\n",
      " [124.83383603]]\n",
      "[[-65.56814165]\n",
      " [124.63342186]]\n",
      "[[-65.10720852]\n",
      " [124.43387149]]\n",
      "[[-64.64848877]\n",
      " [124.23518088]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▏                        | 477/10000 [00:08<02:42, 58.57it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-64.19197191]\n",
      " [124.03734596]]\n",
      "[[-63.73764751]\n",
      " [123.84036271]]\n",
      "[[-63.28550519]\n",
      " [123.64422711]]\n",
      "[[-62.83553462]\n",
      " [123.44893517]]\n",
      "[[-62.38772553]\n",
      " [123.25448293]]\n",
      "[[-61.94206767]\n",
      " [123.06086642]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▎                        | 483/10000 [00:08<02:43, 58.32it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-61.49855086]\n",
      " [122.86808171]]\n",
      "[[-61.05716496]\n",
      " [122.67612488]]\n",
      "[[-60.61789989]\n",
      " [122.48499203]]\n",
      "[[-60.1807456 ]\n",
      " [122.29467929]]\n",
      "[[-59.74569211]\n",
      " [122.10518279]]\n",
      "[[-59.31272945]\n",
      " [121.91649869]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▎                        | 489/10000 [00:08<02:42, 58.48it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-58.88184774]\n",
      " [121.72862316]]\n",
      "[[-58.45303713]\n",
      " [121.54155239]]\n",
      "[[-58.0262878]\n",
      " [121.3552826]]\n",
      "[[-57.60159   ]\n",
      " [121.16981001]]\n",
      "[[-57.17893401]\n",
      " [120.98513088]]\n",
      "[[-56.75831017]\n",
      " [120.80124146]]\n",
      "[[-56.33970885]\n",
      " [120.61813803]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▎                        | 496/10000 [00:08<02:40, 59.19it/s, loss_val=539.9680, thetas=0.6165 -0.7123]\u001b[A\u001b[A\u001b[A[[-55.92312049]\n",
      " [120.43581691]]\n",
      "[[-55.50853554]\n",
      " [120.25427439]]\n",
      "[[-55.09594452]\n",
      " [120.07350682]]\n",
      "[[-54.68533799]\n",
      " [119.89351056]]\n",
      "[[-54.27670657]\n",
      " [119.71428196]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▎                        | 496/10000 [00:08<02:40, 59.19it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-53.87004088]\n",
      " [119.53581741]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▎                        | 502/10000 [00:08<02:43, 58.23it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-53.46533163]\n",
      " [119.35811332]]\n",
      "[[-53.06256956]\n",
      " [119.18116611]]\n",
      "[[-52.66174544]\n",
      " [119.00497222]]\n",
      "[[-52.2628501 ]\n",
      " [118.82952809]]\n",
      "[[-51.8658744]\n",
      " [118.6548302]]\n",
      "[[-51.47080926]\n",
      " [118.48087504]]\n",
      "[[-51.07764563]\n",
      " [118.30765912]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▎                        | 509/10000 [00:08<02:40, 59.16it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-50.6863745 ]\n",
      " [118.13517894]]\n",
      "[[-50.29698693]\n",
      " [117.96343105]]\n",
      "[[-49.90947398]\n",
      " [117.79241201]]\n",
      "[[-49.52382678]\n",
      " [117.62211838]]\n",
      "[[-49.14003649]\n",
      " [117.45254675]]\n",
      "[[-48.75809433]\n",
      " [117.28369371]]\n",
      "[[-48.37799155]\n",
      " [117.1155559 ]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▎                        | 516/10000 [00:08<02:38, 59.67it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-47.99971943]\n",
      " [116.94812994]]\n",
      "[[-47.6232693 ]\n",
      " [116.78141248]]\n",
      "[[-47.24863254]\n",
      " [116.6154002 ]]\n",
      "[[-46.87580057]\n",
      " [116.45008976]]\n",
      "[[-46.50476483]\n",
      " [116.28547787]]\n",
      "[[-46.13551682]\n",
      " [116.12156125]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▎                        | 522/10000 [00:08<02:39, 59.25it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-45.76804808]\n",
      " [115.95833661]]\n",
      "[[-45.40235018]\n",
      " [115.79580071]]\n",
      "[[-45.03841473]\n",
      " [115.6339503 ]]\n",
      "[[-44.67623339]\n",
      " [115.47278216]]\n",
      "[[-44.31579786]\n",
      " [115.31229308]]\n",
      "[[-43.95709986]\n",
      " [115.15247987]]\n",
      "[[-43.60013117]\n",
      " [114.99333934]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▍                        | 529/10000 [00:09<02:39, 59.43it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-43.24488361]\n",
      " [114.83486832]]\n",
      "[[-42.89134901]\n",
      " [114.67706368]]\n",
      "[[-42.53951928]\n",
      " [114.51992228]]\n",
      "[[-42.18938633]\n",
      " [114.36344099]]\n",
      "[[-41.84094213]\n",
      " [114.20761671]]\n",
      "[[-41.49417869]\n",
      " [114.05244635]]\n",
      "[[-41.14908804]\n",
      " [113.89792683]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▍                        | 536/10000 [00:09<02:38, 59.71it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-40.80566226]\n",
      " [113.7440551 ]]\n",
      "[[-40.46389348]\n",
      " [113.59082811]]\n",
      "[[-40.12377384]\n",
      " [113.43824281]]\n",
      "[[-39.78529554]\n",
      " [113.28629621]]\n",
      "[[-39.44845079]\n",
      " [113.13498528]]\n",
      "[[-39.11323188]\n",
      " [112.98430705]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▍                        | 542/10000 [00:09<02:38, 59.63it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-38.77963109]\n",
      " [112.83425854]]\n",
      "[[-38.44764077]\n",
      " [112.68483678]]\n",
      "[[-38.11725328]\n",
      " [112.53603883]]\n",
      "[[-37.78846104]\n",
      " [112.38786176]]\n",
      "[[-37.4612565 ]\n",
      " [112.24030264]]\n",
      "[[-37.13563213]\n",
      " [112.09335857]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   5%|█▍                        | 548/10000 [00:09<02:38, 59.58it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-36.81158045]\n",
      " [111.94702666]]\n",
      "[[-36.48909401]\n",
      " [111.80130402]]\n",
      "[[-36.16816541]\n",
      " [111.65618781]]\n",
      "[[-35.84878726]\n",
      " [111.51167515]]\n",
      "[[-35.53095223]\n",
      " [111.36776323]]\n",
      "[[-35.21465299]\n",
      " [111.2244492 ]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▍                        | 554/10000 [00:09<02:40, 58.84it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-34.89988229]\n",
      " [111.08173027]]\n",
      "[[-34.58663288]\n",
      " [110.93960363]]\n",
      "[[-34.27489756]\n",
      " [110.79806649]]\n",
      "[[-33.96466916]\n",
      " [110.6571161 ]]\n",
      "[[-33.65594053]\n",
      " [110.51674969]]\n",
      "[[-33.34870458]\n",
      " [110.37696451]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▍                        | 560/10000 [00:09<02:39, 59.02it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-33.04295425]\n",
      " [110.23775783]]\n",
      "[[-32.73868248]\n",
      " [110.09912694]]\n",
      "[[-32.43588229]\n",
      " [109.96106912]]\n",
      "[[-32.1345467 ]\n",
      " [109.82358169]]\n",
      "[[-31.83466877]\n",
      " [109.68666197]]\n",
      "[[-31.53624161]\n",
      " [109.55030727]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▍                        | 566/10000 [00:09<02:40, 58.80it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-31.23925833]\n",
      " [109.41451496]]\n",
      "[[-30.94371211]\n",
      " [109.27928239]]\n",
      "[[-30.64959614]\n",
      " [109.14460692]]\n",
      "[[-30.35690364]\n",
      " [109.01048594]]\n",
      "[[-30.06562787]\n",
      " [108.87691684]]\n",
      "[[-29.77576212]\n",
      " [108.74389704]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▍                        | 572/10000 [00:09<02:41, 58.37it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-29.48729972]\n",
      " [108.61142394]]\n",
      "[[-29.20023401]\n",
      " [108.47949499]]\n",
      "[[-28.91455839]\n",
      " [108.34810761]]\n",
      "[[-28.63026626]\n",
      " [108.21725928]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-28.34735108]\n",
      " [108.08694746]]\n",
      "[[-28.06580633]\n",
      " [107.95716962]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▌                        | 578/10000 [00:09<02:41, 58.35it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-27.78562551]\n",
      " [107.82792325]]\n",
      "[[-27.50680216]\n",
      " [107.69920587]]\n",
      "[[-27.22932986]\n",
      " [107.57101499]]\n",
      "[[-26.9532022 ]\n",
      " [107.44334813]]\n",
      "[[-26.67841282]\n",
      " [107.31620283]]\n",
      "[[-26.40495538]\n",
      " [107.18957664]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▌                        | 584/10000 [00:09<02:41, 58.33it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-26.13282356]\n",
      " [107.06346713]]\n",
      "[[-25.8620111 ]\n",
      " [106.93787186]]\n",
      "[[-25.59251174]\n",
      " [106.81278843]]\n",
      "[[-25.32431926]\n",
      " [106.68821442]]\n",
      "[[-25.05742748]\n",
      " [106.56414745]]\n",
      "[[-24.79183023]\n",
      " [106.44058514]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▌                        | 590/10000 [00:10<02:41, 58.42it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-24.52752137]\n",
      " [106.31752511]]\n",
      "[[-24.26449482]\n",
      " [106.194965  ]]\n",
      "[[-24.00274448]\n",
      " [106.07290248]]\n",
      "[[-23.74226433]\n",
      " [105.95133519]]\n",
      "[[-23.48304834]\n",
      " [105.83026083]]\n",
      "[[-23.22509052]\n",
      " [105.70967706]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▌                        | 596/10000 [00:10<02:42, 58.04it/s, loss_val=548.5031, thetas=0.6946 -0.8423]\u001b[A\u001b[A\u001b[A[[-22.96838492]\n",
      " [105.5895816 ]]\n",
      "[[-22.71292561]\n",
      " [105.46997214]]\n",
      "[[-22.45870667]\n",
      " [105.35084641]]\n",
      "[[-22.20572224]\n",
      " [105.23220213]]\n",
      "[[-21.95396646]\n",
      " [105.11403705]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▌                        | 596/10000 [00:10<02:42, 58.04it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-21.70343352]\n",
      " [104.99634891]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▌                        | 602/10000 [00:10<02:45, 56.96it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-21.45411763]\n",
      " [104.87913548]]\n",
      "[[-21.20601302]\n",
      " [104.76239453]]\n",
      "[[-20.95911395]\n",
      " [104.64612383]]\n",
      "[[-20.71341471]\n",
      " [104.53032119]]\n",
      "[[-20.46890962]\n",
      " [104.41498441]]\n",
      "[[-20.22559302]\n",
      " [104.3001113 ]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▌                        | 608/10000 [00:10<02:43, 57.52it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-19.98345929]\n",
      " [104.18569968]]\n",
      "[[-19.74250281]\n",
      " [104.07174739]]\n",
      "[[-19.50271801]\n",
      " [103.95825227]]\n",
      "[[-19.26409935]\n",
      " [103.84521218]]\n",
      "[[-19.02664129]\n",
      " [103.73262498]]\n",
      "[[-18.79033835]\n",
      " [103.62048855]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▌                        | 614/10000 [00:10<02:43, 57.25it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-18.55518504]\n",
      " [103.50880077]]\n",
      "[[-18.32117592]\n",
      " [103.39755954]]\n",
      "[[-18.08830557]\n",
      " [103.28676277]]\n",
      "[[-17.8565686 ]\n",
      " [103.17640836]]\n",
      "[[-17.62595964]\n",
      " [103.06649424]]\n",
      "[[-17.39647334]\n",
      " [102.95701835]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▌                        | 620/10000 [00:10<02:42, 57.56it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-17.16810439]\n",
      " [102.84797863]]\n",
      "[[-16.94084749]\n",
      " [102.73937304]]\n",
      "[[-16.71469737]\n",
      " [102.63119954]]\n",
      "[[-16.4896488 ]\n",
      " [102.52345609]]\n",
      "[[-16.26569655]\n",
      " [102.4161407 ]]\n",
      "[[-16.04283542]\n",
      " [102.30925134]]\n",
      "[[-15.82106026]\n",
      " [102.20278602]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▋                        | 627/10000 [00:10<02:40, 58.52it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-15.60036591]\n",
      " [102.09674275]]\n",
      "[[-15.38074725]\n",
      " [101.99111956]]\n",
      "[[-15.16219919]\n",
      " [101.88591446]]\n",
      "[[-14.94471666]\n",
      " [101.78112551]]\n",
      "[[-14.72829461]\n",
      " [101.67675075]]\n",
      "[[-14.51292801]\n",
      " [101.57278824]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▋                        | 633/10000 [00:10<02:40, 58.45it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-14.29861186]\n",
      " [101.46923605]]\n",
      "[[-14.08534119]\n",
      " [101.36609224]]\n",
      "[[-13.87311105]\n",
      " [101.26335492]]\n",
      "[[-13.6619165 ]\n",
      " [101.16102216]]\n",
      "[[-13.45175263]\n",
      " [101.05909209]]\n",
      "[[-13.24261457]\n",
      " [100.9575628 ]]\n",
      "[[-13.03449745]\n",
      " [100.85643242]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▋                        | 640/10000 [00:10<02:38, 59.02it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-12.82739644]\n",
      " [100.75569908]]\n",
      "[[-12.62130673]\n",
      " [100.65536092]]\n",
      "[[-12.41622351]\n",
      " [100.55541608]]\n",
      "[[-12.21214203]\n",
      " [100.45586273]]\n",
      "[[-12.00905753]\n",
      " [100.35669903]]\n",
      "[[-11.80696529]\n",
      " [100.25792315]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   6%|█▋                        | 646/10000 [00:11<02:39, 58.74it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-11.60586062]\n",
      " [100.15953328]]\n",
      "[[-11.40573883]\n",
      " [100.0615276 ]]\n",
      "[[-11.20659526]\n",
      " [ 99.96390432]]\n",
      "[[-11.00842529]\n",
      " [ 99.86666165]]\n",
      "[[-10.81122429]\n",
      " [ 99.7697978 ]]\n",
      "[[-10.61498767]\n",
      " [ 99.67331099]]\n",
      "[[-10.41971087]\n",
      " [ 99.57719947]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▋                        | 653/10000 [00:11<02:37, 59.22it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-10.22538934]\n",
      " [ 99.48146147]]\n",
      "[[-10.03201855]\n",
      " [ 99.38609525]]\n",
      "[[-9.839594  ]\n",
      " [99.29109905]]\n",
      "[[-9.6481112 ]\n",
      " [99.19647116]]\n",
      "[[-9.4575657 ]\n",
      " [99.10220984]]\n",
      "[[-9.26795304]\n",
      " [99.00831337]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▋                        | 659/10000 [00:11<02:38, 58.76it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-9.07926881]\n",
      " [98.91478006]]\n",
      "[[-8.89150861]\n",
      " [98.82160819]]\n",
      "[[-8.70466806]\n",
      " [98.72879608]]\n",
      "[[-8.51874281]\n",
      " [98.63634204]]\n",
      "[[-8.33372851]\n",
      " [98.54424439]]\n",
      "[[-8.14962085]\n",
      " [98.45250147]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▋                        | 665/10000 [00:11<02:40, 58.28it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-7.96641554]\n",
      " [98.36111162]]\n",
      "[[-7.78410829]\n",
      " [98.27007317]]\n",
      "[[-7.60269485]\n",
      " [98.1793845 ]]\n",
      "[[-7.42217099]\n",
      " [98.08904395]]\n",
      "[[-7.24253249]\n",
      " [97.9990499 ]]\n",
      "[[-7.06377516]\n",
      " [97.90940073]]\n",
      "[[-6.88589481]\n",
      " [97.82009483]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▋                        | 672/10000 [00:11<02:38, 58.75it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-6.7088873 ]\n",
      " [97.73113058]]\n",
      "[[-6.53274849]\n",
      " [97.64250639]]\n",
      "[[-6.35747425]\n",
      " [97.55422067]]\n",
      "[[-6.1830605 ]\n",
      " [97.46627183]]\n",
      "[[-6.00950315]\n",
      " [97.37865829]]\n",
      "[[-5.83679815]\n",
      " [97.2913785 ]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▊                        | 678/10000 [00:11<02:39, 58.61it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-5.66494145]\n",
      " [97.20443088]]\n",
      "[[-5.49392904]\n",
      " [97.11781388]]\n",
      "[[-5.32375692]\n",
      " [97.03152596]]\n",
      "[[-5.1544211 ]\n",
      " [96.94556557]]\n",
      "[[-4.98591762]\n",
      " [96.85993119]]\n",
      "[[-4.81824254]\n",
      " [96.77462129]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▊                        | 684/10000 [00:11<02:38, 58.86it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-4.65139192]\n",
      " [96.68963435]]\n",
      "[[-4.48536187]\n",
      " [96.60496886]]\n",
      "[[-4.32014848]\n",
      " [96.52062331]]\n",
      "[[-4.1557479 ]\n",
      " [96.43659622]]\n",
      "[[-3.99215627]\n",
      " [96.35288608]]\n",
      "[[-3.82936975]\n",
      " [96.26949143]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▊                        | 690/10000 [00:11<02:40, 58.17it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-3.66738454]\n",
      " [96.18641078]]\n",
      "[[-3.50619682]\n",
      " [96.10364267]]\n",
      "[[-3.34580283]\n",
      " [96.02118563]]\n",
      "[[-3.1861988 ]\n",
      " [95.93903821]]\n",
      "[[-3.02738098]\n",
      " [95.85719897]]\n",
      "[[-2.86934565]\n",
      " [95.77566645]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▊                        | 696/10000 [00:11<02:39, 58.50it/s, loss_val=565.8042, thetas=0.7313 -0.9541]\u001b[A\u001b[A\u001b[A[[-2.7120891 ]\n",
      " [95.69443924]]\n",
      "[[-2.55560764]\n",
      " [95.61351589]]\n",
      "[[-2.3998976]\n",
      " [95.532895 ]]\n",
      "[[-2.24495531]\n",
      " [95.45257515]]\n",
      "[[-2.09077715]\n",
      " [95.37255492]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▊                        | 696/10000 [00:11<02:39, 58.50it/s, loss_val=588.3852, thetas=0.7424 -1.0540]\u001b[A\u001b[A\u001b[A[[-1.93735948]\n",
      " [95.29283293]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▊                        | 702/10000 [00:12<02:42, 57.10it/s, loss_val=588.3852, thetas=0.7424 -1.0540]\u001b[A\u001b[A\u001b[A[[-1.7846987 ]\n",
      " [95.21340777]]\n",
      "[[-1.63279123]\n",
      " [95.13427807]]\n",
      "[[-1.48163349]\n",
      " [95.05544243]]\n",
      "[[-1.33122194]\n",
      " [94.97689949]]\n",
      "[[-1.18155303]\n",
      " [94.89864787]]\n",
      "[[-1.03262324]\n",
      " [94.82068622]]\n",
      "\n",
      "\n",
      "\n",
      "Training:   7%|█▊                        | 708/10000 [00:12<02:43, 56.96it/s, loss_val=588.3852, thetas=0.7424 -1.0540]\u001b[A\u001b[A\u001b[A[[-0.88442907]\n",
      " [94.74301319]]\n",
      "Training:   7%|█▊                        | 708/10000 [00:12<02:39, 58.35it/s, loss_val=588.3852, thetas=0.7424 -1.0540]\n",
      "Stop condition detected\n",
      "Final loss: 590.113326517107\n"
     ]
    }
   ],
   "source": [
    "# Step 4. Gradient descent.\n",
    "\n",
    "# now let's find optimal parameters using gradient descent\n",
    "MAX_ITER = 10000\n",
    "thetas = np.random.randn(2, 1)\n",
    "alpha = 1e-5\n",
    "\n",
    "progress = tqdm.tqdm(range(MAX_ITER), \"Training\", file=sys.stdout)\n",
    "loss_val = loss_fn(x_train, y_train, thetas)\n",
    "progress.set_postfix(loss_val=loss_val)\n",
    "\n",
    "for iter in progress:\n",
    "    gradient = gradient_fn(x_train, y_train, thetas)\n",
    "    print(gradient)\n",
    "    thetas_2 = thetas - alpha * gradient\n",
    "    \n",
    "    # TODO: add stop conditions\n",
    "    if abs(thetas_2[0] - thetas[0]) < 0.00001:\n",
    "        progress.close()\n",
    "        loss_val = loss_fn(x_train, y_train, thetas)\n",
    "        print(\"Stop condition detected\")\n",
    "        print(\"Final loss:\", loss_val)\n",
    "        break\n",
    "    \n",
    "    if iter % 100 == 0:\n",
    "        loss_val = loss_fn(x_train, y_train, thetas_2)\n",
    "        progress.set_postfix(loss_val=f\"{loss_val:8.4f}\", thetas=f\"{thetas_2[0][0]:5.4f} {thetas_2[1][0]:5.4f}\")\n",
    "    thetas = thetas_2\n",
    "    \n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  11.8 , predicted: 10.954133590404647\n",
      "Target:  11.0 , predicted: 9.409757572654923\n",
      "Target:  23.7 , predicted: -1.4123795356000173\n",
      "Target:  35.4 , predicted: -4.117094123295022\n",
      "Target:  15.2 , predicted: 7.624574152281781\n",
      "Target:  24.4 , predicted: -2.4995998851054146\n",
      "Target:  33.4 , predicted: -5.239316289135996\n",
      "Target:  31.6 , predicted: -2.875037138135994\n",
      "Target:  13.4 , predicted: 5.79073588207401\n",
      "Target:  34.9 , predicted: -4.012104667898637\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y_hat = predict_fn(x_train, thetas)\n",
    "    print(\"Target: \", y_train[i][0], \", predicted:\", y_hat[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
