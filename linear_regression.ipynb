{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1     0\n",
      "0  23.98  6.459  11.8\n",
      "1  21.52  6.193  11.0\n",
      "2   7.74  6.750  23.7\n",
      "3   4.81  7.249  35.4\n",
      "4  18.06  5.454  15.2\n",
      "5   5.90  6.487  24.4\n",
      "6   2.94  6.998  33.4\n",
      "7   6.36  7.163  31.6\n",
      "8  17.44  6.749  13.4\n",
      "9   4.56  6.975  34.9\n",
      "Shape of train features: (354, 2)\n",
      "Shape of train targets: (354, 1)\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Parse and visualize data\n",
    "# parse train data: read CSV files with train features (train_x) and train targets (train_y)\n",
    "x_train = pd.read_csv(\"D:\\\\Dataset\\\\train\\\\train_x.csv\", header=None)\n",
    "y_train = pd.read_csv(\"D:\\\\Dataset\\\\train\\\\train_y.csv\", header=None)\n",
    "\n",
    "# show first 10 samples\n",
    "print(pd.concat([x_train, y_train], axis=1).head(10))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "print(\"Shape of train features:\", x_train.shape)\n",
    "print(\"Shape of train targets:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Prototypes.\n",
    "\n",
    "# In this demo we will use linear regression to predict targets from features.\n",
    "# In linear regression model with parameters thetas \n",
    "# the prediction y is calculated from features x using linear combination of x and thetas.\n",
    "# For example, for the case of 2 features: \n",
    "# y = theta_0 * x_o + theta_1 * x_1\n",
    "\n",
    "# Let's define some helper functions\n",
    "\n",
    "def predict_fn(x, thetas):\n",
    "    '''\n",
    "    Predict target from features x using parameters thetas and linear regression\n",
    "    \n",
    "    param x: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return y_hat: predicted scalar value for each input samples, shape Nx1\n",
    "    '''    \n",
    "    # TODO: calculate y_hat using linear regression\n",
    "    y_hat = np.zeros((x.shape[0], 1))\n",
    "    for i in range(len(x)):\n",
    "        y_hat[i] = thetas[0] * x[i][0] + thetas[1] * x[i][1]\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def loss_fn(x_train, y_train, thetas):\n",
    "    '''\n",
    "    Calculate average loss value for train dataset (x_train, y_train).\n",
    "    \n",
    "    param x_train: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param y_train: input tagrets, shape Nx1\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return loss: predicted scalar value for each input samples, shape Mx1\n",
    "    '''\n",
    "    y_predicted = predict_fn(x_train, thetas)    \n",
    "    loss = np.mean(np.power(y_train - y_predicted, 2))   \n",
    "    return loss\n",
    "\n",
    "\n",
    "def gradient_fn(x_train, y_train, thetas):\n",
    "    '''\n",
    "    Calculate gradient value for linear regression.\n",
    "    \n",
    "    param x_train: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param y_train: input tagrets, shape Nx1\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return g: predicted scalar value for each input samples, shape Mx1\n",
    "    '''  \n",
    "    # TODO: calculate vector gradient\n",
    "    g = np.zeros_like(thetas)\n",
    "    for i in range(len(x_train)):\n",
    "        g[0] += -2 * x_train[i][0] * (y_train[i] - x_train[i][0] * thetas[0] - x_train[i][1] * thetas[1])\n",
    "        g[1] += -2 * x_train[i][1] * (y_train[i] - x_train[i][0] * thetas[0] - x_train[i][1] * thetas[1])\n",
    "    g[0] = g[0] / len(x_train)\n",
    "    g[1] = g[1] / len(x_train)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                             | 0/1000000 [00:00<?, ?it/s, loss_val=1e+3][[-646.55463344]\n",
      " [-389.33786238]]\n",
      "Training:   0%|                                  | 0/1000000 [00:00<?, ?it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[-319.47505032]\n",
      " [-259.95767539]]\n",
      "[[-147.60204606]\n",
      " [-190.60441113]]\n",
      "[[ -57.49408377]\n",
      " [-152.90366086]]\n",
      "[[ -10.45663975]\n",
      " [-131.90501522]]\n",
      "[[  13.89737385]\n",
      " [-119.73088825]]\n",
      "[[  26.30926219]\n",
      " [-112.23022153]]\n",
      "[[  32.43818317]\n",
      " [-107.21539387]]\n",
      "Training:   0%|                        | 8/1000000 [00:00<3:30:17, 79.26it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[  35.26562689]\n",
      " [-103.53328745]]\n",
      "[[  36.36228314]\n",
      " [-100.57585259]]\n",
      "[[ 36.55552609]\n",
      " [-98.02223483]]\n",
      "[[ 36.28109722]\n",
      " [-95.70290712]]\n",
      "[[ 35.76839932]\n",
      " [-93.52808379]]\n",
      "[[ 35.13812372]\n",
      " [-91.45000322]]\n",
      "[[ 34.45368784]\n",
      " [-89.44305522]]\n",
      "[[ 33.74833652]\n",
      " [-87.4933103 ]]\n",
      "[[ 33.03942091]\n",
      " [-85.59300275]]\n",
      "Training:   0%|                       | 17/1000000 [00:00<3:23:16, 81.99it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[ 32.33592171]\n",
      " [-83.7376236 ]]\n",
      "[[ 31.6424129 ]\n",
      " [-81.92438886]]\n",
      "[[ 30.96115008]\n",
      " [-80.15143218]]\n",
      "[[ 30.29317066]\n",
      " [-78.41737937]]\n",
      "[[ 29.63887348]\n",
      " [-76.72112398]]\n",
      "[[ 28.99832416]\n",
      " [-75.06170891]]\n",
      "[[ 28.37141584]\n",
      " [-73.43826383]]\n",
      "[[ 27.75795386]\n",
      " [-71.84997205]]\n",
      "[[ 27.15770029]\n",
      " [-70.29605288]]\n",
      "[[ 26.57039734]\n",
      " [-68.77575218]]\n",
      "Training:   0%|                       | 27/1000000 [00:00<3:17:15, 84.49it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[ 25.99577959]\n",
      " [-67.28833723]]\n",
      "[[ 25.43358045]\n",
      " [-65.83309381]]\n",
      "[[ 24.88353541]\n",
      " [-64.40932459]]\n",
      "[[ 24.34538376]\n",
      " [-63.01634803]]\n",
      "[[ 23.81886941]\n",
      " [-61.65349776]]\n",
      "[[ 23.30374129]\n",
      " [-60.320122  ]]\n",
      "[[ 22.79975345]\n",
      " [-59.0155832 ]]\n",
      "[[ 22.30666514]\n",
      " [-57.73925763]]\n",
      "[[ 21.82424071]\n",
      " [-56.49053508]]\n",
      "Training:   0%|                       | 36/1000000 [00:00<3:16:22, 84.87it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[ 21.35224958]\n",
      " [-55.26881859]]\n",
      "[[ 20.89046615]\n",
      " [-54.07352408]]\n",
      "[[ 20.43866965]\n",
      " [-52.90408011]]\n",
      "[[ 19.99664412]\n",
      " [-51.75992762]]\n",
      "[[ 19.56417824]\n",
      " [-50.64051963]]\n",
      "[[ 19.14106525]\n",
      " [-49.54532099]]\n",
      "[[ 18.7271029 ]\n",
      " [-48.47380813]]\n",
      "[[ 18.32209329]\n",
      " [-47.4254688 ]]\n",
      "[[ 17.92584278]\n",
      " [-46.39980182]]\n",
      "Training:   0%|                       | 45/1000000 [00:00<3:15:11, 85.38it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[ 17.53816195]\n",
      " [-45.39631686]]\n",
      "[[ 17.15886547]\n",
      " [-44.4145342 ]]\n",
      "[[ 16.787772  ]\n",
      " [-43.45398447]]\n",
      "[[ 16.42470414]\n",
      " [-42.51420848]]\n",
      "[[ 16.06948832]\n",
      " [-41.59475695]]\n",
      "[[ 15.72195473]\n",
      " [-40.69519033]]\n",
      "[[ 15.38193722]\n",
      " [-39.81507857]]\n",
      "[[ 15.04927324]\n",
      " [-38.95400092]]\n",
      "[[ 14.72380377]\n",
      " [-38.11154573]]\n",
      "Training:   0%|                       | 54/1000000 [00:00<3:14:47, 85.56it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[ 14.40537319]\n",
      " [-37.28731025]]\n",
      "[[ 14.09382929]\n",
      " [-36.48090045]]\n",
      "[[ 13.78902312]\n",
      " [-35.69193081]]\n",
      "[[ 13.49080898]\n",
      " [-34.92002416]]\n",
      "[[ 13.19904428]\n",
      " [-34.16481147]]\n",
      "[[ 12.91358957]\n",
      " [-33.42593171]]\n",
      "[[ 12.63430835]\n",
      " [-32.70303165]]\n",
      "[[ 12.36106714]\n",
      " [-31.99576569]]\n",
      "[[ 12.09373528]\n",
      " [-31.30379572]]\n",
      "Training:   0%|                       | 63/1000000 [00:00<3:15:45, 85.13it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[ 11.832185  ]\n",
      " [-30.62679093]]\n",
      "[[ 11.57629124]\n",
      " [-29.96442767]]\n",
      "[[ 11.32593168]\n",
      " [-29.31638929]]\n",
      "[[ 11.08098662]\n",
      " [-28.68236598]]\n",
      "[[ 10.84133897]\n",
      " [-28.06205465]]\n",
      "[[ 10.60687417]\n",
      " [-27.45515875]]\n",
      "[[ 10.37748011]\n",
      " [-26.86138813]]\n",
      "[[ 10.15304714]\n",
      " [-26.28045895]]\n",
      "[[  9.93346797]\n",
      " [-25.71209348]]\n",
      "Training:   0%|                       | 72/1000000 [00:00<3:15:19, 85.32it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[  9.71863762]\n",
      " [-25.15602001]]\n",
      "[[  9.50845339]\n",
      " [-24.61197269]]\n",
      "[[  9.3028148 ]\n",
      " [-24.07969144]]\n",
      "[[  9.10162354]\n",
      " [-23.5589218 ]]\n",
      "[[  8.90478342]\n",
      " [-23.04941481]]\n",
      "[[  8.71220035]\n",
      " [-22.55092688]]\n",
      "[[  8.52378226]\n",
      " [-22.06321971]]\n",
      "[[  8.33943908]\n",
      " [-21.58606015]]\n",
      "[[  8.15908267]\n",
      " [-21.11922008]]\n",
      "Training:   0%|                       | 81/1000000 [00:00<3:15:01, 85.46it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[  7.98262681]\n",
      " [-20.66247633]]\n",
      "[[  7.80998715]\n",
      " [-20.21561054]]\n",
      "[[  7.64108116]\n",
      " [-19.77840908]]\n",
      "[[  7.47582808]\n",
      " [-19.35066294]]\n",
      "[[  7.31414892]\n",
      " [-18.93216764]]\n",
      "[[  7.15596638]\n",
      " [-18.5227231 ]]\n",
      "[[  7.00120485]\n",
      " [-18.12213358]]\n",
      "[[  6.84979033]\n",
      " [-17.73020759]]\n",
      "[[  6.70165044]\n",
      " [-17.34675775]]\n",
      "Training:   0%|                       | 90/1000000 [00:01<3:14:14, 85.79it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[  6.55671436]\n",
      " [-16.97160075]]\n",
      "[[  6.4149128 ]\n",
      " [-16.60455724]]\n",
      "[[  6.27617797]\n",
      " [-16.24545175]]\n",
      "[[  6.14044355]\n",
      " [-15.89411261]]\n",
      "[[  6.00764465]\n",
      " [-15.55037186]]\n",
      "[[  5.87771778]\n",
      " [-15.21406516]]\n",
      "[[  5.75060083]\n",
      " [-14.88503174]]\n",
      "[[  5.62623302]\n",
      " [-14.5631143 ]]\n",
      "[[  5.50455491]\n",
      " [-14.24815894]]\n",
      "Training:   0%|                       | 99/1000000 [00:01<3:16:29, 84.82it/s, loss_val=565.1414, thetas=0.6941 -0.9062][[  5.38550833]\n",
      " [-13.9400151 ]]\n",
      "[[  5.26903635]\n",
      " [-13.63853546]]\n",
      "Training:   0%|                        | 99/1000000 [00:01<3:16:29, 84.82it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[  5.1550833]\n",
      " [-13.3435759]]\n",
      "[[  5.04359471]\n",
      " [-13.05499541]]\n",
      "[[  4.93451728]\n",
      " [-12.77265602]]\n",
      "[[  4.82779885]\n",
      " [-12.49642277]]\n",
      "[[  4.72338841]\n",
      " [-12.22616359]]\n",
      "[[  4.62123606]\n",
      " [-11.96174929]]\n",
      "[[  4.52129294]\n",
      " [-11.70305345]]\n",
      "Training:   0%|                       | 108/1000000 [00:01<3:20:49, 82.98it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[  4.42351128]\n",
      " [-11.44995241]]\n",
      "[[  4.32784434]\n",
      " [-11.20232517]]\n",
      "[[  4.23424638]\n",
      " [-10.96005334]]\n",
      "[[  4.14267266]\n",
      " [-10.72302111]]\n",
      "[[  4.0530794 ]\n",
      " [-10.49111515]]\n",
      "[[  3.96542376]\n",
      " [-10.26422461]]\n",
      "[[  3.87966384]\n",
      " [-10.04224101]]\n",
      "[[ 3.79575865]\n",
      " [-9.82505824]]\n",
      "[[ 3.71366807]\n",
      " [-9.61257245]]\n",
      "Training:   0%|                       | 117/1000000 [00:01<3:24:25, 81.52it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[ 3.63335286]\n",
      " [-9.40468209]]\n",
      "[[ 3.55477461]\n",
      " [-9.20128775]]\n",
      "[[ 3.47789577]\n",
      " [-9.0022922 ]]\n",
      "[[ 3.40267959]\n",
      " [-8.80760032]]\n",
      "[[ 3.3290901 ]\n",
      " [-8.61711902]]\n",
      "[[ 3.25709212]\n",
      " [-8.43075725]]\n",
      "[[ 3.18665124]\n",
      " [-8.24842591]]\n",
      "[[ 3.11773378]\n",
      " [-8.07003783]]\n",
      "[[ 3.05030679]\n",
      " [-7.89550774]]\n",
      "Training:   0%|                       | 126/1000000 [00:01<3:24:42, 81.40it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[ 2.98433804]\n",
      " [-7.7247522 ]]\n",
      "[[ 2.91979599]\n",
      " [-7.55768958]]\n",
      "[[ 2.85664978]\n",
      " [-7.39424001]]\n",
      "[[ 2.79486924]\n",
      " [-7.23432534]]\n",
      "[[ 2.73442481]\n",
      " [-7.07786914]]\n",
      "[[ 2.67528762]\n",
      " [-6.92479661]]\n",
      "[[ 2.61742938]\n",
      " [-6.77503457]]\n",
      "[[ 2.56082243]\n",
      " [-6.62851141]]\n",
      "[[ 2.50543972]\n",
      " [-6.4851571 ]]\n",
      "Training:   0%|                       | 135/1000000 [00:01<3:21:35, 82.67it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[ 2.45125477]\n",
      " [-6.34490311]]\n",
      "[[ 2.39824167]\n",
      " [-6.20768238]]\n",
      "[[ 2.34637508]\n",
      " [-6.0734293 ]]\n",
      "[[ 2.29563021]\n",
      " [-5.94207971]]\n",
      "[[ 2.24598279]\n",
      " [-5.81357081]]\n",
      "[[ 2.19740909]\n",
      " [-5.68784116]]\n",
      "[[ 2.14988589]\n",
      " [-5.56483066]]\n",
      "[[ 2.10339048]\n",
      " [-5.44448049]]\n",
      "[[ 2.05790061]\n",
      " [-5.32673313]]\n",
      "Training:   0%|                       | 144/1000000 [00:01<3:19:23, 83.57it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[ 2.01339455]\n",
      " [-5.21153228]]\n",
      "[[ 1.96985102]\n",
      " [-5.09882287]]\n",
      "[[ 1.92724921]\n",
      " [-4.98855102]]\n",
      "[[ 1.88556874]\n",
      " [-4.88066401]]\n",
      "[[ 1.84478969]\n",
      " [-4.77511026]]\n",
      "[[ 1.80489256]\n",
      " [-4.67183932]]\n",
      "[[ 1.76585829]\n",
      " [-4.57080181]]\n",
      "[[ 1.72766821]\n",
      " [-4.47194943]]\n",
      "[[ 1.69030406]\n",
      " [-4.37523492]]\n",
      "Training:   0%|                       | 153/1000000 [00:01<3:18:58, 83.75it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[ 1.65374798]\n",
      " [-4.28061205]]\n",
      "[[ 1.6179825 ]\n",
      " [-4.18803558]]\n",
      "[[ 1.58299052]\n",
      " [-4.09746126]]\n",
      "[[ 1.5487553 ]\n",
      " [-4.00884578]]\n",
      "[[ 1.51526049]\n",
      " [-3.92214678]]\n",
      "[[ 1.48249007]\n",
      " [-3.83732281]]\n",
      "[[ 1.45042837]\n",
      " [-3.75433332]]\n",
      "[[ 1.41906007]\n",
      " [-3.67313864]]\n",
      "[[ 1.38837016]\n",
      " [-3.59369995]]\n",
      "Training:   0%|                       | 162/1000000 [00:01<3:18:07, 84.11it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[ 1.35834399]\n",
      " [-3.51597928]]\n",
      "[[ 1.32896719]\n",
      " [-3.43993947]]\n",
      "[[ 1.30022571]\n",
      " [-3.36554416]]\n",
      "[[ 1.27210583]\n",
      " [-3.29275779]]\n",
      "[[ 1.2445941 ]\n",
      " [-3.22154557]]\n",
      "[[ 1.21767736]\n",
      " [-3.15187345]]\n",
      "[[ 1.19134274]\n",
      " [-3.08370813]]\n",
      "[[ 1.16557766]\n",
      " [-3.01701701]]\n",
      "[[ 1.1403698 ]\n",
      " [-2.95176821]]\n",
      "Training:   0%|                       | 171/1000000 [00:02<3:18:38, 83.89it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[ 1.11570711]\n",
      " [-2.88793055]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0915778]\n",
      " [-2.8254735]]\n",
      "[[ 1.06797033]\n",
      " [-2.7643672 ]]\n",
      "[[ 1.04487342]\n",
      " [-2.70458244]]\n",
      "[[ 1.02227602]\n",
      " [-2.64609064]]\n",
      "[[ 1.00016734]\n",
      " [-2.58886384]]\n",
      "[[ 0.97853679]\n",
      " [-2.53287468]]\n",
      "[[ 0.95737405]\n",
      " [-2.4780964 ]]\n",
      "[[ 0.936669 ]\n",
      " [-2.4245028]]\n",
      "Training:   0%|                       | 180/1000000 [00:02<3:17:53, 84.21it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[ 0.91641173]\n",
      " [-2.37206826]]\n",
      "[[ 0.89659256]\n",
      " [-2.32076772]]\n",
      "[[ 0.87720203]\n",
      " [-2.27057666]]\n",
      "[[ 0.85823084]\n",
      " [-2.22147107]]\n",
      "[[ 0.83966995]\n",
      " [-2.17342748]]\n",
      "[[ 0.82151047]\n",
      " [-2.12642293]]\n",
      "[[ 0.80374373]\n",
      " [-2.08043495]]\n",
      "[[ 0.78636123]\n",
      " [-2.03544154]]\n",
      "[[ 0.76935465]\n",
      " [-1.9914212 ]]\n",
      "Training:   0%|                       | 189/1000000 [00:02<3:17:54, 84.19it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[ 0.75271588]\n",
      " [-1.94835289]]\n",
      "[[ 0.73643695]\n",
      " [-1.90621601]]\n",
      "[[ 0.72051008]\n",
      " [-1.86499042]]\n",
      "[[ 0.70492767]\n",
      " [-1.82465642]]\n",
      "[[ 0.68968225]\n",
      " [-1.78519471]]\n",
      "[[ 0.67476655]\n",
      " [-1.74658644]]\n",
      "[[ 0.66017342]\n",
      " [-1.70881315]]\n",
      "[[ 0.6458959 ]\n",
      " [-1.67185678]]\n",
      "[[ 0.63192716]\n",
      " [-1.63569966]]\n",
      "Training:   0%|                       | 198/1000000 [00:02<3:16:49, 84.66it/s, loss_val=32.7292, thetas=-0.4176 4.2684][[ 0.61826052]\n",
      " [-1.60032451]]\n",
      "[[ 0.60488945]\n",
      " [-1.56571442]]\n",
      "[[ 0.59180755]\n",
      " [-1.53185283]]\n",
      "Training:   0%|                       | 198/1000000 [00:02<3:16:49, 84.66it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.57900857]\n",
      " [-1.49872357]]\n",
      "[[ 0.5664864 ]\n",
      " [-1.46631079]]\n",
      "[[ 0.55423504]\n",
      " [-1.434599  ]]\n",
      "[[ 0.54224864]\n",
      " [-1.40357303]]\n",
      "[[ 0.53052147]\n",
      " [-1.37321807]]\n",
      "[[ 0.51904793]\n",
      " [-1.34351959]]\n",
      "Training:   0%|                       | 207/1000000 [00:02<3:19:23, 83.57it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.50782252]\n",
      " [-1.31446339]]\n",
      "[[ 0.49683988]\n",
      " [-1.28603559]]\n",
      "[[ 0.48609476]\n",
      " [-1.2582226 ]]\n",
      "[[ 0.47558203]\n",
      " [-1.23101112]]\n",
      "[[ 0.46529665]\n",
      " [-1.20438813]]\n",
      "[[ 0.45523371]\n",
      " [-1.17834092]]\n",
      "[[ 0.44538841]\n",
      " [-1.15285703]]\n",
      "[[ 0.43575603]\n",
      " [-1.12792428]]\n",
      "[[ 0.42633197]\n",
      " [-1.10353075]]\n",
      "Training:   0%|                       | 216/1000000 [00:02<3:16:44, 84.69it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.41711172]\n",
      " [-1.07966478]]\n",
      "[[ 0.40809087]\n",
      " [-1.05631495]]\n",
      "[[ 0.39926512]\n",
      " [-1.03347011]]\n",
      "[[ 0.39063025]\n",
      " [-1.01111933]]\n",
      "[[ 0.38218212]\n",
      " [-0.98925193]]\n",
      "[[ 0.37391669]\n",
      " [-0.96785745]]\n",
      "[[ 0.36583003]\n",
      " [-0.94692567]]\n",
      "[[ 0.35791825]\n",
      " [-0.92644658]]\n",
      "[[ 0.35017758]\n",
      " [-0.90641039]]\n",
      "Training:   0%|                       | 225/1000000 [00:02<3:14:07, 85.84it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.34260431]\n",
      " [-0.88680753]]\n",
      "[[ 0.33519484]\n",
      " [-0.86762861]]\n",
      "[[ 0.3279456 ]\n",
      " [-0.84886447]]\n",
      "[[ 0.32085315]\n",
      " [-0.83050614]]\n",
      "[[ 0.31391408]\n",
      " [-0.81254485]]\n",
      "[[ 0.30712509]\n",
      " [-0.79497201]]\n",
      "[[ 0.30048292]\n",
      " [-0.77777921]]\n",
      "[[ 0.2939844 ]\n",
      " [-0.76095824]]\n",
      "[[ 0.28762642]\n",
      " [-0.74450105]]\n",
      "Training:   0%|                       | 234/1000000 [00:02<3:13:36, 86.06it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.28140594]\n",
      " [-0.72839978]]\n",
      "[[ 0.27532   ]\n",
      " [-0.71264674]]\n",
      "[[ 0.26936567]\n",
      " [-0.69723438]]\n",
      "[[ 0.26354012]\n",
      " [-0.68215535]]\n",
      "[[ 0.25784056]\n",
      " [-0.66740242]]\n",
      "[[ 0.25226426]\n",
      " [-0.65296856]]\n",
      "[[ 0.24680856]\n",
      " [-0.63884686]]\n",
      "[[ 0.24147085]\n",
      " [-0.62503057]]\n",
      "[[ 0.23624858]\n",
      " [-0.61151308]]\n",
      "Training:   0%|                       | 243/1000000 [00:02<3:14:21, 85.73it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.23113925]\n",
      " [-0.59828794]]\n",
      "[[ 0.22614042]\n",
      " [-0.58534881]]\n",
      "[[ 0.2212497 ]\n",
      " [-0.57268951]]\n",
      "[[ 0.21646475]\n",
      " [-0.560304  ]]\n",
      "[[ 0.21178328]\n",
      " [-0.54818635]]\n",
      "[[ 0.20720306]\n",
      " [-0.53633077]]\n",
      "[[ 0.2027219 ]\n",
      " [-0.52473158]]\n",
      "[[ 0.19833765]\n",
      " [-0.51338325]]\n",
      "[[ 0.19404821]\n",
      " [-0.50228035]]\n",
      "Training:   0%|                       | 252/1000000 [00:02<3:15:26, 85.25it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.18985155]\n",
      " [-0.49141757]]\n",
      "[[ 0.18574564]\n",
      " [-0.48078972]]\n",
      "[[ 0.18172854]\n",
      " [-0.47039172]]\n",
      "[[ 0.17779831]\n",
      " [-0.46021859]]\n",
      "[[ 0.17395308]\n",
      " [-0.45026548]]\n",
      "[[ 0.17019101]\n",
      " [-0.44052763]]\n",
      "[[ 0.1665103 ]\n",
      " [-0.43100037]]\n",
      "[[ 0.16290919]\n",
      " [-0.42167916]]\n",
      "[[ 0.15938597]\n",
      " [-0.41255953]]\n",
      "Training:   0%|                       | 261/1000000 [00:03<3:16:45, 84.68it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.15593894]\n",
      " [-0.40363714]]\n",
      "[[ 0.15256646]\n",
      " [-0.39490771]]\n",
      "[[ 0.14926692]\n",
      " [-0.38636707]]\n",
      "[[ 0.14603873]\n",
      " [-0.37801114]]\n",
      "[[ 0.14288036]\n",
      " [-0.36983592]]\n",
      "[[ 0.1397903 ]\n",
      " [-0.36183751]]\n",
      "[[ 0.13676707]\n",
      " [-0.35401208]]\n",
      "[[ 0.13380922]\n",
      " [-0.34635589]]\n",
      "[[ 0.13091534]\n",
      " [-0.33886527]]\n",
      "Training:   0%|                       | 270/1000000 [00:03<3:15:27, 85.25it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.12808404]\n",
      " [-0.33153666]]\n",
      "[[ 0.12531398]\n",
      " [-0.32436654]]\n",
      "[[ 0.12260382]\n",
      " [-0.31735149]]\n",
      "[[ 0.11995228]\n",
      " [-0.31048816]]\n",
      "[[ 0.11735808]\n",
      " [-0.30377325]]\n",
      "[[ 0.11481998]\n",
      " [-0.29720357]]\n",
      "[[ 0.11233678]\n",
      " [-0.29077597]]\n",
      "[[ 0.10990728]\n",
      " [-0.28448738]]\n",
      "[[ 0.10753033]\n",
      " [-0.2783348 ]]\n",
      "Training:   0%|                       | 279/1000000 [00:03<3:14:32, 85.65it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.10520478]\n",
      " [-0.27231527]]\n",
      "[[ 0.10292952]\n",
      " [-0.26642593]]\n",
      "[[ 0.10070347]\n",
      " [-0.26066396]]\n",
      "[[ 0.09852556]\n",
      " [-0.2550266 ]]\n",
      "[[ 0.09639476]\n",
      " [-0.24951116]]\n",
      "[[ 0.09431004]\n",
      " [-0.244115  ]]\n",
      "[[ 0.0922704 ]\n",
      " [-0.23883554]]\n",
      "[[ 0.09027488]\n",
      " [-0.23367026]]\n",
      "[[ 0.08832251]\n",
      " [-0.22861669]]\n",
      "Training:   0%|                       | 288/1000000 [00:03<3:14:27, 85.68it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.08641236]\n",
      " [-0.22367242]]\n",
      "[[ 0.08454353]\n",
      " [-0.21883507]]\n",
      "[[ 0.08271511]\n",
      " [-0.21410234]]\n",
      "[[ 0.08092624]\n",
      " [-0.20947197]]\n",
      "[[ 0.07917606]\n",
      " [-0.20494173]]\n",
      "[[ 0.07746372]\n",
      " [-0.20050947]]\n",
      "[[ 0.07578842]\n",
      " [-0.19617307]]\n",
      "[[ 0.07414935]\n",
      " [-0.19193045]]\n",
      "[[ 0.07254573]\n",
      " [-0.18777959]]\n",
      "Training:   0%|                       | 297/1000000 [00:03<3:16:37, 84.74it/s, loss_val=28.0581, thetas=-0.6291 4.8161][[ 0.07097679]\n",
      " [-0.18371849]]\n",
      "[[ 0.06944178]\n",
      " [-0.17974523]]\n",
      "[[ 0.06793996]\n",
      " [-0.17585789]]\n",
      "[[ 0.06647063]\n",
      " [-0.17205462]]\n",
      "Training:   0%|                       | 297/1000000 [00:03<3:16:37, 84.74it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.06503308]\n",
      " [-0.16833361]]\n",
      "[[ 0.06362661]\n",
      " [-0.16469307]]\n",
      "[[ 0.06225056]\n",
      " [-0.16113127]]\n",
      "[[ 0.06090428]\n",
      " [-0.1576465 ]]\n",
      "[[ 0.0595871 ]\n",
      " [-0.15423709]]\n",
      "Training:   0%|                       | 306/1000000 [00:03<3:20:21, 83.16it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.05829842]\n",
      " [-0.15090141]]\n",
      "[[ 0.0570376 ]\n",
      " [-0.14763788]]\n",
      "[[ 0.05580405]\n",
      " [-0.14444493]]\n",
      "[[ 0.05459718]\n",
      " [-0.14132103]]\n",
      "[[ 0.05341642]\n",
      " [-0.13826469]]\n",
      "[[ 0.05226118]\n",
      " [-0.13527445]]\n",
      "[[ 0.05113093]\n",
      " [-0.13234888]]\n",
      "[[ 0.05002513]\n",
      " [-0.12948658]]\n",
      "[[ 0.04894324]\n",
      " [-0.12668618]]\n",
      "Training:   0%|                       | 315/1000000 [00:03<3:18:16, 84.03it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.04788475]\n",
      " [-0.12394635]]\n",
      "[[ 0.04684915]\n",
      " [-0.12126577]]\n",
      "[[ 0.04583595]\n",
      " [-0.11864317]]\n",
      "[[ 0.04484465]\n",
      " [-0.11607728]]\n",
      "[[ 0.0438748 ]\n",
      " [-0.11356689]]\n",
      "[[ 0.04292593]\n",
      " [-0.11111078]]\n",
      "[[ 0.04199757]\n",
      " [-0.1087078 ]]\n",
      "[[ 0.04108929]\n",
      " [-0.10635678]]\n",
      "[[ 0.04020066]\n",
      " [-0.10405661]]\n",
      "Training:   0%|                       | 324/1000000 [00:03<3:19:11, 83.64it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.03933124]\n",
      " [-0.10180619]]\n",
      "[[ 0.03848063]\n",
      " [-0.09960444]]\n",
      "[[ 0.03764841]\n",
      " [-0.0974503 ]]\n",
      "[[ 0.03683419]\n",
      " [-0.09534275]]\n",
      "[[ 0.03603758]\n",
      " [-0.09328078]]\n",
      "[[ 0.0352582]\n",
      " [-0.0912634]]\n",
      "[[ 0.03449567]\n",
      " [-0.08928965]]\n",
      "[[ 0.03374964]\n",
      " [-0.08735859]]\n",
      "[[ 0.03301974]\n",
      " [-0.0854693 ]]\n",
      "Training:   0%|                       | 333/1000000 [00:03<3:19:22, 83.57it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.03230562]\n",
      " [-0.08362086]]\n",
      "[[ 0.03160695]\n",
      " [-0.0818124 ]]\n",
      "[[ 0.03092339]\n",
      " [-0.08004305]]\n",
      "[[ 0.03025461]\n",
      " [-0.07831196]]\n",
      "[[ 0.0296003 ]\n",
      " [-0.07661831]]\n",
      "[[ 0.02896013]\n",
      " [-0.0749613 ]]\n",
      "[[ 0.02833381]\n",
      " [-0.07334011]]\n",
      "[[ 0.02772104]\n",
      " [-0.07175399]]\n",
      "[[ 0.02712152]\n",
      " [-0.07020217]]\n",
      "Training:   0%|                       | 342/1000000 [00:04<3:18:17, 84.03it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.02653496]\n",
      " [-0.06868392]]\n",
      "[[ 0.0259611]\n",
      " [-0.0671985]]\n",
      "[[ 0.02539964]\n",
      " [-0.0657452 ]]\n",
      "[[ 0.02485032]\n",
      " [-0.06432333]]\n",
      "[[ 0.02431288]\n",
      " [-0.06293222]]\n",
      "[[ 0.02378707]\n",
      " [-0.06157119]]\n",
      "[[ 0.02327263]\n",
      " [-0.06023959]]\n",
      "[[ 0.02276932]\n",
      " [-0.05893679]]\n",
      "[[ 0.02227688]\n",
      " [-0.05766217]]\n",
      "Training:   0%|                       | 351/1000000 [00:04<3:17:37, 84.30it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.0217951 ]\n",
      " [-0.05641512]]\n",
      "[[ 0.02132374]\n",
      " [-0.05519503]]\n",
      "[[ 0.02086258]\n",
      " [-0.05400133]]\n",
      "[[ 0.02041138]\n",
      " [-0.05283345]]\n",
      "[[ 0.01996995]\n",
      " [-0.05169083]]\n",
      "[[ 0.01953806]\n",
      " [-0.05057291]]\n",
      "[[ 0.01911551]\n",
      " [-0.04947918]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0187021 ]\n",
      " [-0.04840909]]\n",
      "[[ 0.01829763]\n",
      " [-0.04736215]]\n",
      "Training:   0%|                       | 360/1000000 [00:04<3:16:25, 84.82it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.01790191]\n",
      " [-0.04633786]]\n",
      "[[ 0.01751475]\n",
      " [-0.04533571]]\n",
      "[[ 0.01713596]\n",
      " [-0.04435524]]\n",
      "[[ 0.01676536]\n",
      " [-0.04339597]]\n",
      "[[ 0.01640278]\n",
      " [-0.04245745]]\n",
      "[[ 0.01604803]\n",
      " [-0.04153923]]\n",
      "[[ 0.01570097]\n",
      " [-0.04064086]]\n",
      "[[ 0.0153614 ]\n",
      " [-0.03976192]]\n",
      "[[ 0.01502918]\n",
      " [-0.038902  ]]\n",
      "Training:   0%|                       | 369/1000000 [00:04<3:16:53, 84.62it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.01470415]\n",
      " [-0.03806066]]\n",
      "[[ 0.01438614]\n",
      " [-0.03723753]]\n",
      "[[ 0.01407501]\n",
      " [-0.0364322 ]]\n",
      "[[ 0.01377061]\n",
      " [-0.03564428]]\n",
      "[[ 0.0134728]\n",
      " [-0.0348734]]\n",
      "[[ 0.01318142]\n",
      " [-0.0341192 ]]\n",
      "[[ 0.01289635]\n",
      " [-0.03338131]]\n",
      "[[ 0.01261744]\n",
      " [-0.03265937]]\n",
      "[[ 0.01234456]\n",
      " [-0.03195305]]\n",
      "Training:   0%|                       | 378/1000000 [00:04<3:18:21, 83.99it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.01207759]\n",
      " [-0.031262  ]]\n",
      "[[ 0.01181639]\n",
      " [-0.0305859 ]]\n",
      "[[ 0.01156084]\n",
      " [-0.02992442]]\n",
      "[[ 0.01131081]\n",
      " [-0.02927725]]\n",
      "[[ 0.01106619]\n",
      " [-0.02864407]]\n",
      "[[ 0.01082687]\n",
      " [-0.02802459]]\n",
      "[[ 0.01059271]\n",
      " [-0.0274185 ]]\n",
      "[[ 0.01036363]\n",
      " [-0.02682553]]\n",
      "[[ 0.01013949]\n",
      " [-0.02624537]]\n",
      "Training:   0%|                       | 387/1000000 [00:04<3:17:40, 84.28it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.00992021]\n",
      " [-0.02567777]]\n",
      "[[ 0.00970566]\n",
      " [-0.02512244]]\n",
      "[[ 0.00949576]\n",
      " [-0.02457911]]\n",
      "[[ 0.0092904 ]\n",
      " [-0.02404754]]\n",
      "[[ 0.00908947]\n",
      " [-0.02352747]]\n",
      "[[ 0.0088929 ]\n",
      " [-0.02301864]]\n",
      "[[ 0.00870057]\n",
      " [-0.02252082]]\n",
      "[[ 0.0085124 ]\n",
      " [-0.02203376]]\n",
      "[[ 0.00832831]\n",
      " [-0.02155724]]\n",
      "Training:   0%|                       | 396/1000000 [00:04<3:16:58, 84.58it/s, loss_val=27.9992, thetas=-0.6529 4.8776][[ 0.00814819]\n",
      " [-0.02109102]]\n",
      "[[ 0.00797197]\n",
      " [-0.02063489]]\n",
      "[[ 0.00779956]\n",
      " [-0.02018862]]\n",
      "[[ 0.00763088]\n",
      " [-0.019752  ]]\n",
      "[[ 0.00746585]\n",
      " [-0.01932483]]\n",
      "Training:   0%|                       | 396/1000000 [00:04<3:16:58, 84.58it/s, loss_val=27.9984, thetas=-0.6556 4.8845][[ 0.00730438]\n",
      " [-0.01890689]]\n",
      "[[ 0.00714641]\n",
      " [-0.01849799]]\n",
      "[[ 0.00699186]\n",
      " [-0.01809794]]\n",
      "[[ 0.00684065]\n",
      " [-0.01770654]]\n",
      "Training:   0%|                       | 405/1000000 [00:04<3:20:02, 83.28it/s, loss_val=27.9984, thetas=-0.6556 4.8845][[ 0.0066927]\n",
      " [-0.0173236]]\n",
      "[[ 0.00654796]\n",
      " [-0.01694894]]\n",
      "[[ 0.00640635]\n",
      " [-0.01658239]]\n",
      "[[ 0.0062678 ]\n",
      " [-0.01622376]]\n",
      "[[ 0.00613225]\n",
      " [-0.01587289]]\n",
      "[[ 0.00599962]\n",
      " [-0.01552961]]\n",
      "[[ 0.00586987]\n",
      " [-0.01519375]]\n",
      "[[ 0.00574292]\n",
      " [-0.01486516]]\n",
      "[[ 0.00561872]\n",
      " [-0.01454367]]\n",
      "Training:   0%|                       | 414/1000000 [00:04<3:19:57, 83.31it/s, loss_val=27.9984, thetas=-0.6556 4.8845][[ 0.00549721]\n",
      " [-0.01422914]]\n",
      "[[ 0.00537832]\n",
      " [-0.0139214 ]]\n",
      "[[ 0.005262  ]\n",
      " [-0.01362033]]\n",
      "[[ 0.0051482 ]\n",
      " [-0.01332576]]\n",
      "[[ 0.00503686]\n",
      " [-0.01303757]]\n",
      "[[ 0.00492793]\n",
      " [-0.0127556 ]]\n",
      "[[ 0.00482135]\n",
      " [-0.01247974]]\n",
      "[[ 0.00471708]\n",
      " [-0.01220984]]\n",
      "[[ 0.00461507]\n",
      " [-0.01194578]]\n",
      "Training:   0%|                       | 423/1000000 [00:05<3:19:50, 83.37it/s, loss_val=27.9984, thetas=-0.6556 4.8845][[ 0.00451526]\n",
      " [-0.01168743]]\n",
      "[[ 0.00441761]\n",
      " [-0.01143467]]\n",
      "[[ 0.00432207]\n",
      " [-0.01118737]]\n",
      "[[ 0.00422859]\n",
      " [-0.01094542]]\n",
      "[[ 0.00413714]\n",
      " [-0.01070871]]\n",
      "[[ 0.00404767]\n",
      " [-0.01047711]]\n",
      "[[ 0.00396013]\n",
      " [-0.01025052]]\n",
      "[[ 0.00387448]\n",
      " [-0.01002883]]\n",
      "[[ 0.00379069]\n",
      " [-0.00981194]]\n",
      "Training:   0%|                       | 423/1000000 [00:05<3:21:28, 82.69it/s, loss_val=27.9984, thetas=-0.6556 4.8845]\n",
      "Stop condition detected\n",
      "Final loss: 27.99842865378782\n"
     ]
    }
   ],
   "source": [
    "# Step 4. Gradient descent.\n",
    "\n",
    "# now let's find optimal parameters using gradient descent\n",
    "MAX_ITER = 1000000\n",
    "thetas = np.random.randn(2, 1)\n",
    "alpha = 1e-3\n",
    "\n",
    "progress = tqdm.tqdm(range(MAX_ITER), \"Training\", file=sys.stdout)\n",
    "loss_val = loss_fn(x_train, y_train, thetas)\n",
    "progress.set_postfix(loss_val=loss_val)\n",
    "\n",
    "for iter in progress:\n",
    "    gradient = gradient_fn(x_train, y_train, thetas)\n",
    "    print(gradient)\n",
    "    thetas_2 = thetas - alpha * gradient\n",
    "    \n",
    "    # TODO: add stop conditions\n",
    "    if (abs(thetas_2[0] - thetas[0]) < 0.00001) and (abs(thetas_2[1] -thetas[1]) < 0.00001):\n",
    "        progress.close()\n",
    "        loss_val = loss_fn(x_train, y_train, thetas)\n",
    "        print(\"Stop condition detected\")\n",
    "        print(\"Final loss:\", loss_val)\n",
    "        break\n",
    "    \n",
    "    if iter % 100 == 0:\n",
    "        loss_val = loss_fn(x_train, y_train, thetas_2)\n",
    "        progress.set_postfix(loss_val=f\"{loss_val:8.4f}\", thetas=f\"{thetas_2[0][0]:5.4f} {thetas_2[1][0]:5.4f}\")\n",
    "    thetas = thetas_2\n",
    "    \n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  11.8 , predicted: 15.825663061797137\n",
      "Target:  11.0 , predicted: 16.139711609970796\n",
      "Target:  23.7 , predicted: 27.899383727052204\n",
      "Target:  35.4 , predicted: 32.259012900433305\n",
      "Target:  15.2 , predicted: 14.798901476444952\n",
      "Target:  24.4 , predicted: 27.821420233616394\n",
      "Target:  33.4 , predicted: 32.259351209667344\n",
      "Target:  31.6 , predicted: 30.822201690957264\n",
      "Target:  13.4 , predicted: 21.53211004080499\n",
      "Target:  34.9 , predicted: 31.084403500872533\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y_hat = predict_fn(x_train, thetas)\n",
    "    print(\"Target: \", y_train[i][0], \", predicted:\", y_hat[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
