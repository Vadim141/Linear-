{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1     0\n",
      "0  23.98  6.459  11.8\n",
      "1  21.52  6.193  11.0\n",
      "2   7.74  6.750  23.7\n",
      "3   4.81  7.249  35.4\n",
      "4  18.06  5.454  15.2\n",
      "5   5.90  6.487  24.4\n",
      "6   2.94  6.998  33.4\n",
      "7   6.36  7.163  31.6\n",
      "8  17.44  6.749  13.4\n",
      "9   4.56  6.975  34.9\n",
      "Shape of train features: (354, 2)\n",
      "Shape of train targets: (354, 1)\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Parse and visualize data\n",
    "# parse train data: read CSV files with train features (train_x) and train targets (train_y)\n",
    "x_train = pd.read_csv(\"D:\\\\Dataset\\\\train\\\\train_x.csv\", header=None)\n",
    "y_train = pd.read_csv(\"D:\\\\Dataset\\\\train\\\\train_y.csv\", header=None)\n",
    "\n",
    "# show first 10 samples\n",
    "print(pd.concat([x_train, y_train], axis=1).head(10))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "print(\"Shape of train features:\", x_train.shape)\n",
    "print(\"Shape of train targets:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Prototypes.\n",
    "\n",
    "# In this demo we will use linear regression to predict targets from features.\n",
    "# In linear regression model with parameters thetas \n",
    "# the prediction y is calculated from features x using linear combination of x and thetas.\n",
    "# For example, for the case of 2 features: \n",
    "# y = theta_0 * x_o + theta_1 * x_1\n",
    "\n",
    "# Let's define some helper functions\n",
    "\n",
    "def predict_fn(x, thetas):\n",
    "    '''\n",
    "    Predict target from features x using parameters thetas and linear regression\n",
    "    \n",
    "    param x: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return y_hat: predicted scalar value for each input samples, shape Nx1\n",
    "    '''    \n",
    "    # TODO: calculate y_hat using linear regression\n",
    "    y_hat = np.zeros((x.shape[0], 1))\n",
    "    for i in range(len(x)):\n",
    "        y_hat[i] = thetas[0] * x[i][0] + thetas[1] * x[i][1]\n",
    "    #print(y_hat)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def loss_fn(x_train, y_train, thetas):\n",
    "    '''\n",
    "    Calculate average loss value for train dataset (x_train, y_train).\n",
    "    \n",
    "    param x_train: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param y_train: input tagrets, shape Nx1\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return loss: predicted scalar value for each input samples, shape Mx1\n",
    "    '''\n",
    "    y_predicted = predict_fn(x_train, thetas)    \n",
    "    loss = np.mean(np.power(y_train - y_predicted, 2))   \n",
    "    return loss\n",
    "\n",
    "\n",
    "def gradient_fn(x_train, y_train, thetas):\n",
    "    '''\n",
    "    Calculate gradient value for linear regression.\n",
    "    \n",
    "    param x_train: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param y_train: input tagrets, shape Nx1\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return g: predicted scalar value for each input samples, shape Mx1\n",
    "    '''  \n",
    "    # TODO: calculate vector gradient\n",
    "    g = np.zeros_like(thetas)\n",
    "    for i in range(len(x_train)):\n",
    "        g[0] += -2 * x_train[i][0] * (y_train[i] - x_train[i][0] * thetas[0] - x_train[i][1] * thetas[1])\n",
    "        g[1] += -2 * x_train[i][1] * (y_train[i] - x_train[i][0] * thetas[0] - x_train[i][1] * thetas[1])\n",
    "    g[0] = g[0] / len(x_train)\n",
    "    g[1] = g[1] / len(x_train)\n",
    "    print(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                              | 0/100000 [00:00<?, ?it/s, loss_val=1e+3][[-826.16659546]\n",
      " [-389.20010103]]\n",
      "Training:   0%|                                   | 0/100000 [00:00<?, ?it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[-424.66128093]\n",
      " [-232.57036652]]\n",
      "[[-213.34676743]\n",
      " [-149.4506457 ]]\n",
      "[[-102.2346524 ]\n",
      " [-105.07460858]]\n",
      "[[-43.9121241 ]\n",
      " [-81.12435871]]\n",
      "[[-13.39851516]\n",
      " [-67.94832525]]\n",
      "[[  2.46783825]\n",
      " [-60.46104043]]\n",
      "[[ 10.621428  ]\n",
      " [-55.98282204]]\n",
      "[[ 14.71578754]\n",
      " [-53.10154919]]\n",
      "Training:   0%|                           | 9/100000 [00:00<19:14, 86.59it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[ 16.67578728]\n",
      " [-51.07295687]]\n",
      "[[ 17.51550501]\n",
      " [-49.50466407]]\n",
      "[[ 17.76914389]\n",
      " [-48.18969505]]\n",
      "[[ 17.71807359]\n",
      " [-47.01876521]]\n",
      "[[ 17.51045334]\n",
      " [-45.93406633]]\n",
      "[[ 17.22425861]\n",
      " [-44.9049169 ]]\n",
      "[[ 16.90048843]\n",
      " [-43.91493271]]\n",
      "[[ 16.56066161]\n",
      " [-42.95526724]]\n",
      "[[ 16.21603502]\n",
      " [-42.02105003]]\n",
      "[[ 15.87246051]\n",
      " [-41.10951004]]\n",
      "Training:   0%|                          | 19/100000 [00:00<18:37, 89.50it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[ 15.53294383]\n",
      " [-40.21898682]]\n",
      "[[ 15.19899291]\n",
      " [-39.34840937]]\n",
      "[[ 14.87132807]\n",
      " [-38.49702154]]\n",
      "[[ 14.55025633]\n",
      " [-37.66423718]]\n",
      "[[ 14.23586848]\n",
      " [-36.84956379]]\n",
      "[[ 13.92814294]\n",
      " [-36.05256219]]\n",
      "[[ 13.6270004 ]\n",
      " [-35.27282515]]\n",
      "[[ 13.33233264]\n",
      " [-34.50996611]]\n",
      "[[ 13.04401761]\n",
      " [-33.7636131 ]]\n",
      "[[ 12.76192739]\n",
      " [-33.03340549]]\n",
      "Training:   0%|                          | 29/100000 [00:00<18:25, 90.42it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[ 12.48593236]\n",
      " [-32.31899217]]\n",
      "[[ 12.21590332]\n",
      " [-31.62003054]]\n",
      "[[ 11.95171263]\n",
      " [-30.9361859 ]]\n",
      "[[ 11.69323477]\n",
      " [-30.26713103]]\n",
      "[[ 11.44034655]\n",
      " [-29.61254592]]\n",
      "[[ 11.19292731]\n",
      " [-28.97211756]]\n",
      "[[ 10.95085886]\n",
      " [-28.34553975]]\n",
      "[[ 10.71402554]\n",
      " [-27.7325129 ]]\n",
      "[[ 10.48231417]\n",
      " [-27.13274396]]\n",
      "[[ 10.25561398]\n",
      " [-26.54594618]]\n",
      "Training:   0%|                          | 39/100000 [00:00<18:17, 91.08it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[ 10.03381662]\n",
      " [-25.97183903]]\n",
      "[[  9.81681604]\n",
      " [-25.41014806]]\n",
      "[[  9.60450851]\n",
      " [-24.86060473]]\n",
      "[[  9.39679255]\n",
      " [-24.32294634]]\n",
      "[[  9.19356883]\n",
      " [-23.79691584]]\n",
      "[[  8.99474023]\n",
      " [-23.28226177]]\n",
      "[[  8.80021167]\n",
      " [-22.77873807]]\n",
      "[[  8.60989017]\n",
      " [-22.28610405]]\n",
      "[[  8.42368473]\n",
      " [-21.80412419]]\n",
      "[[  8.24150635]\n",
      " [-21.33256807]]\n",
      "Training:   0%|                          | 49/100000 [00:00<18:17, 91.04it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[  8.06326792]\n",
      " [-20.87121025]]\n",
      "[[  7.88888424]\n",
      " [-20.41983019]]\n",
      "[[  7.71827195]\n",
      " [-19.97821208]]\n",
      "[[  7.55134947]\n",
      " [-19.54614482]]\n",
      "[[  7.38803702]\n",
      " [-19.12342185]]\n",
      "[[  7.22825651]\n",
      " [-18.70984107]]\n",
      "[[  7.07193156]\n",
      " [-18.30520477]]\n",
      "[[  6.91898744]\n",
      " [-17.90931951]]\n",
      "[[  6.76935102]\n",
      " [-17.52199603]]\n",
      "Training:   0%|                          | 58/100000 [00:00<18:24, 90.47it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[  6.62295079]\n",
      " [-17.14304916]]\n",
      "[[  6.47971674]\n",
      " [-16.77229775]]\n",
      "[[  6.3395804 ]\n",
      " [-16.40956456]]\n",
      "[[  6.20247478]\n",
      " [-16.05467616]]\n",
      "[[  6.06833434]\n",
      " [-15.70746291]]\n",
      "[[  5.93709493]\n",
      " [-15.36775881]]\n",
      "[[  5.80869384]\n",
      " [-15.03540147]]\n",
      "[[  5.68306966]\n",
      " [-14.710232  ]]\n",
      "[[  5.56016235]\n",
      " [-14.39209494]]\n",
      "[[  5.43991314]\n",
      " [-14.0808382 ]]\n",
      "Training:   0%|                          | 68/100000 [00:00<18:13, 91.36it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[  5.32226456]\n",
      " [-13.77631299]]\n",
      "[[  5.20716035]\n",
      " [-13.47837373]]\n",
      "[[  5.09454549]\n",
      " [-13.18687797]]\n",
      "[[  4.98436615]\n",
      " [-12.90168637]]\n",
      "[[  4.87656964]\n",
      " [-12.62266259]]\n",
      "[[  4.77110445]\n",
      " [-12.34967324]]\n",
      "[[  4.66792014]\n",
      " [-12.0825878 ]]\n",
      "[[  4.56696739]\n",
      " [-11.8212786 ]]\n",
      "[[  4.46819793]\n",
      " [-11.56562071]]\n",
      "[[  4.37156456]\n",
      " [-11.31549192]]\n",
      "Training:   0%|                          | 78/100000 [00:00<18:08, 91.84it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[  4.27702107]\n",
      " [-11.07077264]]\n",
      "[[  4.18452226]\n",
      " [-10.83134589]]\n",
      "[[  4.09402392]\n",
      " [-10.59709721]]\n",
      "[[  4.00548278]\n",
      " [-10.3679146 ]]\n",
      "[[  3.91885651]\n",
      " [-10.1436885 ]]\n",
      "[[ 3.8341037 ]\n",
      " [-9.92431173]]\n",
      "[[ 3.75118383]\n",
      " [-9.7096794 ]]\n",
      "[[ 3.67005727]\n",
      " [-9.49968891]]\n",
      "[[ 3.59068522]\n",
      " [-9.29423986]]\n",
      "[[ 3.51302974]\n",
      " [-9.09323405]]\n",
      "Training:   0%|                          | 88/100000 [00:00<18:17, 91.07it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[ 3.43705372]\n",
      " [-8.89657537]]\n",
      "[[ 3.36272082]\n",
      " [-8.70416982]]\n",
      "[[ 3.28999552]\n",
      " [-8.51592541]]\n",
      "[[ 3.21884304]\n",
      " [-8.33175214]]\n",
      "[[ 3.14922936]\n",
      " [-8.15156198]]\n",
      "[[ 3.08112122]\n",
      " [-7.97526877]]\n",
      "[[ 3.01448605]\n",
      " [-7.80278824]]\n",
      "[[ 2.94929199]\n",
      " [-7.63403794]]\n",
      "[[ 2.88550788]\n",
      " [-7.46893719]]\n",
      "[[ 2.82310322]\n",
      " [-7.30740706]]\n",
      "Training:   0%|                          | 98/100000 [00:01<18:22, 90.65it/s, loss_val=365.8445, thetas=-0.5373 1.7634][[ 2.76204818]\n",
      " [-7.14937032]]\n",
      "[[ 2.70231358]\n",
      " [-6.99475144]]\n",
      "[[ 2.64387085]\n",
      " [-6.84347649]]\n",
      "Training:   0%|                           | 98/100000 [00:01<18:22, 90.65it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 2.58669206]\n",
      " [-6.69547314]]\n",
      "[[ 2.53074987]\n",
      " [-6.55067066]]\n",
      "[[ 2.47601753]\n",
      " [-6.40899981]]\n",
      "[[ 2.42246889]\n",
      " [-6.27039286]]\n",
      "[[ 2.37007834]\n",
      " [-6.13478355]]\n",
      "[[ 2.31882084]\n",
      " [-6.00210706]]\n",
      "[[ 2.26867188]\n",
      " [-5.87229995]]\n",
      "Training:   0%|                          | 108/100000 [00:01<18:17, 90.99it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 2.21960749]\n",
      " [-5.74530017]]\n",
      "[[ 2.17160421]\n",
      " [-5.621047  ]]\n",
      "[[ 2.12463909]\n",
      " [-5.49948105]]\n",
      "[[ 2.07868968]\n",
      " [-5.3805442 ]]\n",
      "[[ 2.03373402]\n",
      " [-5.26417958]]\n",
      "[[ 1.98975061]\n",
      " [-5.15033157]]\n",
      "[[ 1.94671843]\n",
      " [-5.03894574]]\n",
      "[[ 1.9046169 ]\n",
      " [-4.92996885]]\n",
      "[[ 1.86342589]\n",
      " [-4.82334879]]\n",
      "[[ 1.82312572]\n",
      " [-4.7190346 ]]\n",
      "Training:   0%|                          | 118/100000 [00:01<18:14, 91.28it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 1.78369712]\n",
      " [-4.6169764 ]]\n",
      "[[ 1.74512124]\n",
      " [-4.5171254 ]]\n",
      "[[ 1.70737964]\n",
      " [-4.41943388]]\n",
      "[[ 1.67045427]\n",
      " [-4.32385512]]\n",
      "[[ 1.63432748]\n",
      " [-4.23034344]]\n",
      "[[ 1.59898201]\n",
      " [-4.13885412]]\n",
      "[[ 1.56440095]\n",
      " [-4.04934344]]\n",
      "[[ 1.53056777]\n",
      " [-3.9617686 ]]\n",
      "[[ 1.4974663 ]\n",
      " [-3.87608774]]\n",
      "[[ 1.46508071]\n",
      " [-3.79225989]]\n",
      "Training:   0%|                          | 128/100000 [00:01<18:12, 91.44it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 1.43339552]\n",
      " [-3.71024497]]\n",
      "[[ 1.40239558]\n",
      " [-3.63000379]]\n",
      "[[ 1.37206608]\n",
      " [-3.55149797]]\n",
      "[[ 1.34239251]\n",
      " [-3.47469   ]]\n",
      "[[ 1.31336069]\n",
      " [-3.39954315]]\n",
      "[[ 1.28495674]\n",
      " [-3.32602149]]\n",
      "[[ 1.25716708]\n",
      " [-3.25408987]]\n",
      "[[ 1.22997843]\n",
      " [-3.18371392]]\n",
      "[[ 1.20337778]\n",
      " [-3.11485999]]\n",
      "[[ 1.17735242]\n",
      " [-3.04749515]]\n",
      "Training:   0%|                          | 138/100000 [00:01<18:16, 91.05it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 1.15188991]\n",
      " [-2.9815872 ]]\n",
      "[[ 1.12697807]\n",
      " [-2.91710465]]\n",
      "[[ 1.102605  ]\n",
      " [-2.85401665]]\n",
      "[[ 1.07875905]\n",
      " [-2.79229305]]\n",
      "[[ 1.05542881]\n",
      " [-2.73190434]]\n",
      "[[ 1.03260313]\n",
      " [-2.67282165]]\n",
      "[[ 1.0102711 ]\n",
      " [-2.61501674]]\n",
      "[[ 0.98842205]\n",
      " [-2.55846198]]\n",
      "[[ 0.96704552]\n",
      " [-2.50313032]]\n",
      "[[ 0.9461313 ]\n",
      " [-2.44899531]]\n",
      "Training:   0%|                          | 148/100000 [00:01<18:15, 91.13it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 0.92566939]\n",
      " [-2.39603108]]\n",
      "[[ 0.90565001]\n",
      " [-2.3442123 ]]\n",
      "[[ 0.88606359]\n",
      " [-2.2935142 ]]\n",
      "[[ 0.86690076]\n",
      " [-2.24391254]]\n",
      "[[ 0.84815236]\n",
      " [-2.19538362]]\n",
      "[[ 0.82980944]\n",
      " [-2.14790422]]\n",
      "[[ 0.81186321]\n",
      " [-2.10145166]]\n",
      "[[ 0.79430511]\n",
      " [-2.05600373]]\n",
      "[[ 0.77712673]\n",
      " [-2.01153869]]\n",
      "[[ 0.76031987]\n",
      " [-1.9680353 ]]\n",
      "Training:   0%|                          | 158/100000 [00:01<18:22, 90.59it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 0.74387649]\n",
      " [-1.92547275]]\n",
      "[[ 0.72778873]\n",
      " [-1.8838307 ]]\n",
      "[[ 0.7120489 ]\n",
      " [-1.84308924]]\n",
      "[[ 0.69664947]\n",
      " [-1.80322889]]\n",
      "[[ 0.68158309]\n",
      " [-1.7642306 ]]\n",
      "[[ 0.66684254]\n",
      " [-1.72607572]]\n",
      "[[ 0.65242079]\n",
      " [-1.68874601]]\n",
      "[[ 0.63831094]\n",
      " [-1.65222363]]\n",
      "[[ 0.62450624]\n",
      " [-1.61649111]]\n",
      "[[ 0.61100009]\n",
      " [-1.58153138]]\n",
      "Training:   0%|                          | 168/100000 [00:01<18:32, 89.72it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 0.59778603]\n",
      " [-1.54732773]]\n",
      "[[ 0.58485776]\n",
      " [-1.51386379]]\n",
      "[[ 0.57220909]\n",
      " [-1.48112357]]\n",
      "[[ 0.55983397]\n",
      " [-1.44909143]]\n",
      "[[ 0.54772648]\n",
      " [-1.41775204]]\n",
      "[[ 0.53588084]\n",
      " [-1.38709042]]\n",
      "[[ 0.52429139]\n",
      " [-1.35709192]]\n",
      "[[ 0.51295258]\n",
      " [-1.3277422 ]]\n",
      "[[ 0.50185899]\n",
      " [-1.29902722]]\n",
      "Training:   0%|                          | 177/100000 [00:01<18:37, 89.29it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 0.49100533]\n",
      " [-1.27093326]]\n",
      "[[ 0.48038639]\n",
      " [-1.24344688]]\n",
      "[[ 0.46999711]\n",
      " [-1.21655495]]\n",
      "[[ 0.45983252]\n",
      " [-1.19024461]]\n",
      "[[ 0.44988776]\n",
      " [-1.16450328]]\n",
      "[[ 0.44015807]\n",
      " [-1.13931866]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4306388]\n",
      " [-1.1146787]]\n",
      "[[ 0.42132541]\n",
      " [-1.09057163]]\n",
      "[[ 0.41221344]\n",
      " [-1.06698592]]\n",
      "Training:   0%|                          | 186/100000 [00:02<18:38, 89.25it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 0.40329853]\n",
      " [-1.0439103 ]]\n",
      "[[ 0.39457642]\n",
      " [-1.02133373]]\n",
      "[[ 0.38604295]\n",
      " [-0.99924543]]\n",
      "[[ 0.37769403]\n",
      " [-0.97763482]]\n",
      "[[ 0.36952567]\n",
      " [-0.95649159]]\n",
      "[[ 0.36153397]\n",
      " [-0.93580562]]\n",
      "[[ 0.3537151 ]\n",
      " [-0.91556702]]\n",
      "[[ 0.34606533]\n",
      " [-0.89576612]]\n",
      "[[ 0.338581  ]\n",
      " [-0.87639346]]\n",
      "Training:   0%|                          | 195/100000 [00:02<18:38, 89.23it/s, loss_val=29.1895, thetas=-0.5363 4.5758][[ 0.33125854]\n",
      " [-0.85743976]]\n",
      "[[ 0.32409443]\n",
      " [-0.83889598]]\n",
      "[[ 0.31708527]\n",
      " [-0.82075324]]\n",
      "[[ 0.31022769]\n",
      " [-0.80300287]]\n",
      "[[ 0.30351842]\n",
      " [-0.78563639]]\n",
      "[[ 0.29695425]\n",
      " [-0.76864549]]\n",
      "Training:   0%|                          | 195/100000 [00:02<18:38, 89.23it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.29053204]\n",
      " [-0.75202206]]\n",
      "[[ 0.28424873]\n",
      " [-0.73575813]]\n",
      "[[ 0.2781013 ]\n",
      " [-0.71984595]]\n",
      "Training:   0%|                          | 204/100000 [00:02<18:55, 87.90it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.27208683]\n",
      " [-0.7042779 ]]\n",
      "[[ 0.26620243]\n",
      " [-0.68904653]]\n",
      "[[ 0.26044529]\n",
      " [-0.67414458]]\n",
      "[[ 0.25481266]\n",
      " [-0.6595649 ]]\n",
      "[[ 0.24930184]\n",
      " [-0.64530054]]\n",
      "[[ 0.24391021]\n",
      " [-0.63134468]]\n",
      "[[ 0.23863518]\n",
      " [-0.61769063]]\n",
      "[[ 0.23347424]\n",
      " [-0.60433189]]\n",
      "[[ 0.22842491]\n",
      " [-0.59126205]]\n",
      "[[ 0.22348478]\n",
      " [-0.57847487]]\n",
      "Training:   0%|                          | 214/100000 [00:02<18:40, 89.03it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.21865149]\n",
      " [-0.56596424]]\n",
      "[[ 0.21392273]\n",
      " [-0.55372417]]\n",
      "[[ 0.20929624]\n",
      " [-0.54174882]]\n",
      "[[ 0.20476981]\n",
      " [-0.53003246]]\n",
      "[[ 0.20034127]\n",
      " [-0.51856949]]\n",
      "[[ 0.1960085 ]\n",
      " [-0.50735443]]\n",
      "[[ 0.19176944]\n",
      " [-0.49638191]]\n",
      "[[ 0.18762206]\n",
      " [-0.4856467 ]]\n",
      "[[ 0.18356437]\n",
      " [-0.47514365]]\n",
      "Training:   0%|                          | 223/100000 [00:02<18:40, 89.07it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.17959444]\n",
      " [-0.46486776]]\n",
      "[[ 0.17571036]\n",
      " [-0.4548141 ]]\n",
      "[[ 0.17191029]\n",
      " [-0.44497787]]\n",
      "[[ 0.1681924 ]\n",
      " [-0.43535437]]\n",
      "[[ 0.16455491]\n",
      " [-0.42593899]]\n",
      "[[ 0.1609961 ]\n",
      " [-0.41672724]]\n",
      "[[ 0.15751425]\n",
      " [-0.40771471]]\n",
      "[[ 0.1541077]\n",
      " [-0.3988971]]\n",
      "[[ 0.15077482]\n",
      " [-0.39027018]]\n",
      "[[ 0.14751403]\n",
      " [-0.38182984]]\n",
      "Training:   0%|                          | 233/100000 [00:02<18:30, 89.87it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.14432375]\n",
      " [-0.37357203]]\n",
      "[[ 0.14120247]\n",
      " [-0.36549282]]\n",
      "[[ 0.1381487 ]\n",
      " [-0.35758833]]\n",
      "[[ 0.13516097]\n",
      " [-0.3498548 ]]\n",
      "[[ 0.13223785]\n",
      " [-0.34228852]]\n",
      "[[ 0.12937795]\n",
      " [-0.33488587]]\n",
      "[[ 0.12657991]\n",
      " [-0.32764332]]\n",
      "[[ 0.12384237]\n",
      " [-0.3205574 ]]\n",
      "[[ 0.12116404]\n",
      " [-0.31362473]]\n",
      "[[ 0.11854364]\n",
      " [-0.30684199]]\n",
      "Training:   0%|                          | 243/100000 [00:02<18:14, 91.19it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.1159799 ]\n",
      " [-0.30020594]]\n",
      "[[ 0.11347162]\n",
      " [-0.29371341]]\n",
      "[[ 0.11101757]\n",
      " [-0.2873613 ]]\n",
      "[[ 0.10861661]\n",
      " [-0.28114656]]\n",
      "[[ 0.10626756]\n",
      " [-0.27506622]]\n",
      "[[ 0.10396932]\n",
      " [-0.26911738]]\n",
      "[[ 0.10172079]\n",
      " [-0.2632972 ]]\n",
      "[[ 0.09952088]\n",
      " [-0.25760289]]\n",
      "[[ 0.09736855]\n",
      " [-0.25203174]]\n",
      "[[ 0.09526276]\n",
      " [-0.24658107]]\n",
      "Training:   0%|                          | 253/100000 [00:02<18:08, 91.62it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.09320252]\n",
      " [-0.24124828]]\n",
      "[[ 0.09118684]\n",
      " [-0.23603082]]\n",
      "[[ 0.08921475]\n",
      " [-0.2309262 ]]\n",
      "[[ 0.08728531]\n",
      " [-0.22593197]]\n",
      "[[ 0.0853976 ]\n",
      " [-0.22104576]]\n",
      "[[ 0.08355071]\n",
      " [-0.21626522]]\n",
      "[[ 0.08174376]\n",
      " [-0.21158807]]\n",
      "[[ 0.0799759 ]\n",
      " [-0.20701207]]\n",
      "[[ 0.07824627]\n",
      " [-0.20253504]]\n",
      "[[ 0.07655404]\n",
      " [-0.19815483]]\n",
      "Training:   0%|                          | 263/100000 [00:02<18:12, 91.26it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.07489841]\n",
      " [-0.19386935]]\n",
      "[[ 0.07327859]\n",
      " [-0.18967655]]\n",
      "[[ 0.0716938 ]\n",
      " [-0.18557443]]\n",
      "[[ 0.07014328]\n",
      " [-0.18156103]]\n",
      "[[ 0.0686263 ]\n",
      " [-0.17763442]]\n",
      "[[ 0.06714212]\n",
      " [-0.17379273]]\n",
      "[[ 0.06569005]\n",
      " [-0.17003413]]\n",
      "[[ 0.06426937]\n",
      " [-0.16635682]]\n",
      "[[ 0.06287942]\n",
      " [-0.16275903]]\n",
      "[[ 0.06151953]\n",
      " [-0.15923905]]\n",
      "Training:   0%|                          | 273/100000 [00:03<18:16, 90.93it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.06018906]\n",
      " [-0.1557952 ]]\n",
      "[[ 0.05888735]\n",
      " [-0.15242583]]\n",
      "[[ 0.0576138 ]\n",
      " [-0.14912933]]\n",
      "[[ 0.05636779]\n",
      " [-0.14590412]]\n",
      "[[ 0.05514873]\n",
      " [-0.14274866]]\n",
      "[[ 0.05395603]\n",
      " [-0.13966145]]\n",
      "[[ 0.05278913]\n",
      " [-0.136641  ]]\n",
      "[[ 0.05164746]\n",
      " [-0.13368588]]\n",
      "[[ 0.05053049]\n",
      " [-0.13079466]]\n",
      "[[ 0.04943767]\n",
      " [-0.12796598]]\n",
      "Training:   0%|                          | 283/100000 [00:03<18:19, 90.69it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.04836848]\n",
      " [-0.12519847]]\n",
      "[[ 0.04732242]\n",
      " [-0.12249081]]\n",
      "[[ 0.04629898]\n",
      " [-0.11984171]]\n",
      "[[ 0.04529768]\n",
      " [-0.1172499 ]]\n",
      "[[ 0.04431803]\n",
      " [-0.11471415]]\n",
      "[[ 0.04335957]\n",
      " [-0.11223323]]\n",
      "[[ 0.04242183]\n",
      " [-0.10980597]]\n",
      "[[ 0.04150438]\n",
      " [-0.10743121]]\n",
      "[[ 0.04060677]\n",
      " [-0.1051078 ]]\n",
      "[[ 0.03972857]\n",
      " [-0.10283464]]\n",
      "Training:   0%|                          | 293/100000 [00:03<18:21, 90.53it/s, loss_val=28.0135, thetas=-0.6425 4.8506][[ 0.03886936]\n",
      " [-0.10061065]]\n",
      "[[ 0.03802874]\n",
      " [-0.09843475]]\n",
      "[[ 0.03720629]\n",
      " [-0.09630591]]\n",
      "[[ 0.03640163]\n",
      " [-0.09422311]]\n",
      "[[ 0.03561438]\n",
      " [-0.09218535]]\n",
      "[[ 0.03484415]\n",
      " [-0.09019167]]\n",
      "[[ 0.03409058]\n",
      " [-0.0882411 ]]\n",
      "[[ 0.0333533 ]\n",
      " [-0.08633271]]\n",
      "Training:   0%|                          | 293/100000 [00:03<18:21, 90.53it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.03263197]\n",
      " [-0.0844656 ]]\n",
      "[[ 0.03192624]\n",
      " [-0.08263887]]\n",
      "Training:   0%|                          | 303/100000 [00:03<18:40, 88.96it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.03123578]\n",
      " [-0.08085165]]\n",
      "[[ 0.03056024]\n",
      " [-0.07910307]]\n",
      "[[ 0.02989932]\n",
      " [-0.07739232]]\n",
      "[[ 0.02925269]\n",
      " [-0.07571856]]\n",
      "[[ 0.02862004]\n",
      " [-0.074081  ]]\n",
      "[[ 0.02800108]\n",
      " [-0.07247886]]\n",
      "[[ 0.0273955 ]\n",
      " [-0.07091136]]\n",
      "[[ 0.02680302]\n",
      " [-0.06937777]]\n",
      "[[ 0.02622336]\n",
      " [-0.06787734]]\n",
      "Training:   0%|                          | 312/100000 [00:03<18:43, 88.76it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.02565623]\n",
      " [-0.06640936]]\n",
      "[[ 0.02510136]\n",
      " [-0.06497313]]\n",
      "[[ 0.0245585 ]\n",
      " [-0.06356796]]\n",
      "[[ 0.02402737]\n",
      " [-0.06219318]]\n",
      "[[ 0.02350773]\n",
      " [-0.06084814]]\n",
      "[[ 0.02299933]\n",
      " [-0.05953218]]\n",
      "[[ 0.02250193]\n",
      " [-0.05824468]]\n",
      "[[ 0.02201528]\n",
      " [-0.05698503]]\n",
      "[[ 0.02153916]\n",
      " [-0.05575262]]\n",
      "Training:   0%|                          | 321/100000 [00:03<18:41, 88.88it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.02107333]\n",
      " [-0.05454686]]\n",
      "[[ 0.02061758]\n",
      " [-0.05336718]]\n",
      "[[ 0.02017169]\n",
      " [-0.05221301]]\n",
      "[[ 0.01973543]\n",
      " [-0.0510838 ]]\n",
      "[[ 0.01930862]\n",
      " [-0.04997902]]\n",
      "[[ 0.01889103]\n",
      " [-0.04889813]]\n",
      "[[ 0.01848248]\n",
      " [-0.04784061]]\n",
      "[[ 0.01808276]\n",
      " [-0.04680596]]\n",
      "[[ 0.01769168]\n",
      " [-0.0457937 ]]\n",
      "[[ 0.01730907]\n",
      " [-0.04480332]]\n",
      "Training:   0%|                          | 331/100000 [00:03<18:36, 89.26it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.01693472]\n",
      " [-0.04383436]]\n",
      "[[ 0.01656848]\n",
      " [-0.04288636]]\n",
      "[[ 0.01621015]\n",
      " [-0.04195886]]\n",
      "[[ 0.01585958]\n",
      " [-0.04105142]]\n",
      "[[ 0.01551658]\n",
      " [-0.0401636 ]]\n",
      "[[ 0.01518101]\n",
      " [-0.03929499]]\n",
      "[[ 0.01485269]\n",
      " [-0.03844516]]\n",
      "[[ 0.01453147]\n",
      " [-0.03761371]]\n",
      "[[ 0.0142172 ]\n",
      " [-0.03680024]]\n",
      "[[ 0.01390973]\n",
      " [-0.03600436]]\n",
      "Training:   0%|                          | 341/100000 [00:03<18:18, 90.74it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.0136089]\n",
      " [-0.0352257]]\n",
      "[[ 0.01331458]\n",
      " [-0.03446387]]\n",
      "[[ 0.01302663]\n",
      " [-0.03371853]]\n",
      "[[ 0.0127449]\n",
      " [-0.0329893]]\n",
      "[[ 0.01246927]\n",
      " [-0.03227584]]\n",
      "[[ 0.0121996 ]\n",
      " [-0.03157781]]\n",
      "[[ 0.01193576]\n",
      " [-0.03089488]]\n",
      "[[ 0.01167762]\n",
      " [-0.03022672]]\n",
      "[[ 0.01142507]\n",
      " [-0.02957301]]\n",
      "[[ 0.01117798]\n",
      " [-0.02893344]]\n",
      "Training:   0%|                          | 351/100000 [00:03<18:08, 91.56it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.01093624]\n",
      " [-0.0283077 ]]\n",
      "[[ 0.01069972]\n",
      " [-0.02769549]]\n",
      "[[ 0.01046832]\n",
      " [-0.02709652]]\n",
      "[[ 0.01024192]\n",
      " [-0.02651051]]\n",
      "[[ 0.01002042]\n",
      " [-0.02593717]]\n",
      "[[ 0.00980371]\n",
      " [-0.02537622]]\n",
      "[[ 0.00959169]\n",
      " [-0.02482741]]\n",
      "[[ 0.00938425]\n",
      " [-0.02429047]]\n",
      "[[ 0.00918129]\n",
      " [-0.02376515]]\n",
      "[[ 0.00898273]\n",
      " [-0.02325118]]\n",
      "Training:   0%|                          | 361/100000 [00:03<18:03, 91.95it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.00878846]\n",
      " [-0.02274833]]\n",
      "[[ 0.0085984 ]\n",
      " [-0.02225635]]\n",
      "[[ 0.00841244]\n",
      " [-0.02177501]]\n",
      "[[ 0.0082305 ]\n",
      " [-0.02130409]]\n",
      "[[ 0.0080525 ]\n",
      " [-0.02084335]]\n",
      "[[ 0.00787835]\n",
      " [-0.02039257]]\n",
      "[[ 0.00770797]\n",
      " [-0.01995154]]\n",
      "[[ 0.00754127]\n",
      " [-0.01952005]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00737817]\n",
      " [-0.01909789]]\n",
      "[[ 0.00721861]\n",
      " [-0.01868486]]\n",
      "Training:   0%|                          | 371/100000 [00:04<18:15, 90.90it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.00706249]\n",
      " [-0.01828077]]\n",
      "[[ 0.00690975]\n",
      " [-0.01788541]]\n",
      "[[ 0.00676031]\n",
      " [-0.0174986 ]]\n",
      "[[ 0.00661411]\n",
      " [-0.01712016]]\n",
      "[[ 0.00647107]\n",
      " [-0.01674991]]\n",
      "[[ 0.00633112]\n",
      " [-0.01638766]]\n",
      "[[ 0.00619419]\n",
      " [-0.01603324]]\n",
      "[[ 0.00606023]\n",
      " [-0.01568649]]\n",
      "[[ 0.00592917]\n",
      " [-0.01534724]]\n",
      "[[ 0.00580094]\n",
      " [-0.01501533]]\n",
      "Training:   0%|                          | 381/100000 [00:04<18:24, 90.18it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.00567548]\n",
      " [-0.01469059]]\n",
      "[[ 0.00555274]\n",
      " [-0.01437288]]\n",
      "[[ 0.00543265]\n",
      " [-0.01406204]]\n",
      "[[ 0.00531516]\n",
      " [-0.01375792]]\n",
      "[[ 0.00520021]\n",
      " [-0.01346038]]\n",
      "[[ 0.00508774]\n",
      " [-0.01316927]]\n",
      "[[ 0.00497771]\n",
      " [-0.01288446]]\n",
      "[[ 0.00487006]\n",
      " [-0.01260581]]\n",
      "[[ 0.00476473]\n",
      " [-0.01233319]]\n",
      "[[ 0.00466169]\n",
      " [-0.01206646]]\n",
      "Training:   0%|                          | 391/100000 [00:04<18:21, 90.42it/s, loss_val=27.9986, thetas=-0.6544 4.8815][[ 0.00456087]\n",
      " [-0.0118055 ]]\n",
      "[[ 0.00446223]\n",
      " [-0.01155018]]\n",
      "[[ 0.00436573]\n",
      " [-0.01130039]]\n",
      "[[ 0.00427131]\n",
      " [-0.01105599]]\n",
      "[[ 0.00417894]\n",
      " [-0.01081689]]\n",
      "[[ 0.00408856]\n",
      " [-0.01058295]]\n",
      "[[ 0.00400014]\n",
      " [-0.01035407]]\n",
      "[[ 0.00391362]\n",
      " [-0.01013015]]\n",
      "[[ 0.00382898]\n",
      " [-0.00991106]]\n",
      "[[ 0.00374618]\n",
      " [-0.00969672]]\n",
      "Training:   0%|                          | 401/100000 [00:04<18:40, 88.89it/s, loss_val=27.9984, thetas=-0.6557 4.8849][[ 0.00366516]\n",
      " [-0.00948701]]\n",
      "[[ 0.00358589]\n",
      " [-0.00928183]]\n",
      "[[ 0.00350834]\n",
      " [-0.00908109]]\n",
      "[[ 0.00343247]\n",
      " [-0.0088847 ]]\n",
      "[[ 0.00335823]\n",
      " [-0.00869255]]\n",
      "[[ 0.0032856 ]\n",
      " [-0.00850456]]\n",
      "[[ 0.00321455]\n",
      " [-0.00832063]]\n",
      "[[ 0.00314502]\n",
      " [-0.00814068]]\n",
      "[[ 0.00307701]\n",
      " [-0.00796462]]\n",
      "Training:   0%|                          | 410/100000 [00:04<18:39, 88.97it/s, loss_val=27.9984, thetas=-0.6557 4.8849][[ 0.00301046]\n",
      " [-0.00779237]]\n",
      "[[ 0.00294535]\n",
      " [-0.00762385]]\n",
      "[[ 0.00288166]\n",
      " [-0.00745897]]\n",
      "[[ 0.00281933]\n",
      " [-0.00729765]]\n",
      "[[ 0.00275836]\n",
      " [-0.00713983]]\n",
      "[[ 0.00269871]\n",
      " [-0.00698541]]\n",
      "[[ 0.00264034]\n",
      " [-0.00683434]]\n",
      "[[ 0.00258324]\n",
      " [-0.00668653]]\n",
      "[[ 0.00252737]\n",
      " [-0.00654193]]\n",
      "[[ 0.00247271]\n",
      " [-0.00640044]]\n",
      "Training:   0%|                          | 420/100000 [00:04<18:22, 90.29it/s, loss_val=27.9984, thetas=-0.6557 4.8849][[ 0.00241923]\n",
      " [-0.00626202]]\n",
      "[[ 0.00236691]\n",
      " [-0.00612659]]\n",
      "[[ 0.00231573]\n",
      " [-0.00599409]]\n",
      "[[ 0.00226564]\n",
      " [-0.00586446]]\n",
      "[[ 0.00221664]\n",
      " [-0.00573763]]\n",
      "[[ 0.00216871]\n",
      " [-0.00561354]]\n",
      "[[ 0.0021218 ]\n",
      " [-0.00549214]]\n",
      "[[ 0.00207591]\n",
      " [-0.00537336]]\n",
      "[[ 0.00203102]\n",
      " [-0.00525715]]\n",
      "[[ 0.00198709]\n",
      " [-0.00514346]]\n",
      "Training:   0%|                          | 430/100000 [00:04<18:05, 91.73it/s, loss_val=27.9984, thetas=-0.6557 4.8849][[ 0.00194412]\n",
      " [-0.00503222]]\n",
      "[[ 0.00190207]\n",
      " [-0.00492339]]\n",
      "[[ 0.00186094]\n",
      " [-0.00481691]]\n",
      "[[ 0.00182069]\n",
      " [-0.00471273]]\n",
      "[[ 0.00178132]\n",
      " [-0.00461081]]\n",
      "[[ 0.00174279]\n",
      " [-0.00451109]]\n",
      "[[ 0.0017051 ]\n",
      " [-0.00441353]]\n",
      "[[ 0.00166822]\n",
      " [-0.00431808]]\n",
      "[[ 0.00163215]\n",
      " [-0.0042247 ]]\n",
      "[[ 0.00159685]\n",
      " [-0.00413333]]\n",
      "Training:   0%|                          | 440/100000 [00:04<18:02, 92.01it/s, loss_val=27.9984, thetas=-0.6557 4.8849][[ 0.00156231]\n",
      " [-0.00404394]]\n",
      "[[ 0.00152852]\n",
      " [-0.00395648]]\n",
      "[[ 0.00149547]\n",
      " [-0.00387091]]\n",
      "[[ 0.00146312]\n",
      " [-0.0037872 ]]\n",
      "[[ 0.00143148]\n",
      " [-0.00370529]]\n",
      "[[ 0.00140052]\n",
      " [-0.00362516]]\n",
      "[[ 0.00137023]\n",
      " [-0.00354676]]\n",
      "[[ 0.0013406 ]\n",
      " [-0.00347005]]\n",
      "[[ 0.00131161]\n",
      " [-0.003395  ]]\n",
      "[[ 0.00128324]\n",
      " [-0.00332158]]\n",
      "Training:   0%|                          | 450/100000 [00:04<17:59, 92.20it/s, loss_val=27.9984, thetas=-0.6557 4.8849][[ 0.00125549]\n",
      " [-0.00324975]]\n",
      "[[ 0.00122834]\n",
      " [-0.00317946]]\n",
      "[[ 0.00120177]\n",
      " [-0.0031107 ]]\n",
      "[[ 0.00117578]\n",
      " [-0.00304343]]\n",
      "[[ 0.00115035]\n",
      " [-0.00297761]]\n",
      "[[ 0.00112547]\n",
      " [-0.00291321]]\n",
      "[[ 0.00110113]\n",
      " [-0.00285021]]\n",
      "[[ 0.00107732]\n",
      " [-0.00278857]]\n",
      "[[ 0.00105402]\n",
      " [-0.00272826]]\n",
      "[[ 0.00103122]\n",
      " [-0.00266925]]\n",
      "Training:   0%|                          | 460/100000 [00:05<18:09, 91.32it/s, loss_val=27.9984, thetas=-0.6557 4.8849][[ 0.00100892]\n",
      " [-0.00261153]]\n",
      "[[ 0.0009871 ]\n",
      " [-0.00255505]]\n",
      "Training:   0%|                          | 460/100000 [00:05<18:23, 90.22it/s, loss_val=27.9984, thetas=-0.6557 4.8849]\n",
      "Stop condition detected\n",
      "Final loss: 27.998426269239243\n"
     ]
    }
   ],
   "source": [
    "# Step 4. Gradient descent.\n",
    "\n",
    "# now let's find optimal parameters using gradient descent\n",
    "MAX_ITER = 100000\n",
    "thetas = np.random.randn(2, 1)\n",
    "alpha = 1e-3\n",
    "\n",
    "progress = tqdm.tqdm(range(MAX_ITER), \"Training\", file=sys.stdout)\n",
    "loss_val = loss_fn(x_train, y_train, thetas)\n",
    "progress.set_postfix(loss_val=loss_val)\n",
    "\n",
    "for iter in progress:\n",
    "    gradient = gradient_fn(x_train, y_train, thetas)\n",
    "    #print(gradient)\n",
    "    thetas_2 = thetas - alpha * gradient\n",
    "    \n",
    "    # TODO: add stop conditions\n",
    "    if abs(thetas_2[0] - thetas[0]) < 0.000001:\n",
    "        progress.close()\n",
    "        loss_val = loss_fn(x_train, y_train, thetas)\n",
    "        print(\"Stop condition detected\")\n",
    "        print(\"Final loss:\", loss_val)\n",
    "        break\n",
    "    \n",
    "    if iter % 100 == 0:\n",
    "        loss_val = loss_fn(x_train, y_train, thetas_2)\n",
    "        progress.set_postfix(loss_val=f\"{loss_val:8.4f}\", thetas=f\"{thetas_2[0][0]:5.4f} {thetas_2[1][0]:5.4f}\")\n",
    "    thetas = thetas_2\n",
    "    \n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  11.8 , predicted: 15.825991165907341\n",
      "Target:  11.0 , predicted: 16.139959669607084\n",
      "Target:  23.7 , predicted: 27.898943989361573\n",
      "Target:  35.4 , predicted: 32.25838240789613\n",
      "Target:  15.2 , predicted: 14.799079627889157\n",
      "Target:  24.4 , predicted: 27.820928115369096\n",
      "Target:  33.4 , predicted: 32.25866557751542\n",
      "Target:  31.6 , predicted: 30.82165129384511\n",
      "Target:  13.4 , predicted: 21.53210871603006\n",
      "Target:  34.9 , predicted: 31.083793758712453\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y_hat = predict_fn(x_train, thetas)\n",
    "    print(\"Target: \", y_train[i][0], \", predicted:\", y_hat[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
